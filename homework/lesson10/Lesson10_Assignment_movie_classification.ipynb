{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from hanziconv import HanziConv\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\data\\datasource-master\\movie_comments.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "comment = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49355</th>\n",
       "      <td>48008</td>\n",
       "      <td>https://movie.douban.com/subject/1291824/</td>\n",
       "      <td>黑鹰坠落 Black Hawk Down</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49364</th>\n",
       "      <td>48017</td>\n",
       "      <td>https://movie.douban.com/subject/1291824/</td>\n",
       "      <td>黑鹰坠落 Black Hawk Down</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                       link                  name  \\\n",
       "49355  48008  https://movie.douban.com/subject/1291824/  黑鹰坠落 Black Hawk Down   \n",
       "49364  48017  https://movie.douban.com/subject/1291824/  黑鹰坠落 Black Hawk Down   \n",
       "\n",
       "      comment star  \n",
       "49355     NaN    3  \n",
       "49364     NaN    3  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_index = comment[comment['comment'].isnull()].index\n",
    "comment.loc[null_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         0\n",
       "link       0\n",
       "name       0\n",
       "comment    0\n",
       "star       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comment.drop(null_index,inplace=True)\n",
    "comment.dropna(inplace=True)\n",
    "comment.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_path = r'C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\data\\stopword.txt'\n",
    "stops = set()\n",
    "with open(stop_words_path, 'r', encoding='utf8') as rf:\n",
    "    for line in rf:\n",
    "        stops.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_hanzi_contained(ss):\n",
    "    pattern = re.compile(r'[\\u4E00-\\u9FA5]')\n",
    "    if pattern.findall(ss):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def convert_to_simplified(ss):\n",
    "    return HanziConv.toSimplified(ss)\n",
    "    \n",
    "def cut(ss):\n",
    "    cuts = ''.join(re.findall(r'\\w+', ss))\n",
    "    return jieba.cut(cuts)\n",
    "    \n",
    "def clear_web_chars(ss, webs=r'\\\\n|&nbsp|\\xa0|\\\\xa0|\\u3000|\\\\u3000|\\\\u0020|\\u0020'):\n",
    "    return re.sub(webs, '', ss)\n",
    "\n",
    "def clear_stop_words(ss, stops):\n",
    "    return [s for s in ss if s not in stops]\n",
    "    \n",
    "def get_cut_hz(ss):\n",
    "    ss = convert_to_simplified(ss)\n",
    "    ss = clear_web_chars(ss)\n",
    "    ss = list(cut(ss))\n",
    "#     ss = clear_stop_words(ss, stops)\n",
    "    if is_hanzi_contained(''.join(ss)):\n",
    "        return (ss)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment['words'] = comment['comment'].apply(get_cut_hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 12038,\n",
       "         '1': 12083,\n",
       "         2: 13517,\n",
       "         '2': 14059,\n",
       "         3: 32851,\n",
       "         '3': 30949,\n",
       "         4: 41419,\n",
       "         '4': 39578,\n",
       "         '5': 30691,\n",
       "         5: 26151})"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(comment['star'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment.drop(comment[comment['star']=='star'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    80997\n",
      "3    63800\n",
      "5    56842\n",
      "2    27576\n",
      "1    24121\n",
      "Name: star, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b3acbe5cf8>"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comment['star'] = pd.to_numeric(comment['star'])\n",
    "star_count = comment['star'].value_counts()\n",
    "print(star_count)\n",
    "star_count.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = [word for word, count in mini_counter.items() if count < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_corpus_w2v_path = os.path.join(os.path.dirname(file_path), 'movie_corpus_w2v.txt')\n",
    "with open(movie_corpus_w2v_path, 'w', encoding='utf-8') as wf:\n",
    "    for words in comment['words']:\n",
    "        try:\n",
    "            wf.write(' '.join(words) + '\\n')   \n",
    "        except:\n",
    "            print(words)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "w2v_model_path = r'C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\model\\word2vec\\word2vec_wiki.model'  # word2vec模型目录的路径, trained by wiki\n",
    "model = Word2Vec.load(w2v_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "sentences = LineSentence(movie_corpus_w2v_path)\n",
    "model.train(sentences,total_examples=comment.shape[0], epochs=5)\n",
    "new_model_path = os.path.join(os.path.dirname(w2v_model_path), 'movie_corpus_w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words = list(chain.from_iterable(comment['words'].tolist()))   \n",
    "comment_tf = Counter(comment_words)\n",
    "with open(comment_tf_path, 'wb') as wf:\n",
    "    pickle.dump(comment_tf, wf, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_wiki_path = os.path.join(os.path.dirname(movie_corpus_w2v_path), 'wiki_tf.pickle')\n",
    "with open(comment_wiki_path, 'rb') as rf:\n",
    "    wiki_tf = pickle.load(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in comment_tf:\n",
    "    wiki_tf[word] += comment_tf[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_path = os.path.join(os.path.dirname(movie_corpus_w2v_path), 'tf.pickle')\n",
    "with open(tf_path, 'wb') as wf:\n",
    "    pickle.dump(wiki_tf, wf, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = wiki_tf\n",
    "sentences = comment['words'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(sentences, w2v_model, tf, a=1e-3, npc=1):\n",
    "    s2v = np.zeros([len(sentences), w2v_model.vector_size])\n",
    "    invalid_indecis = list()\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        w2v = np.zeros(w2v_model.vector_size)\n",
    "        j = 0\n",
    "        for j, word in enumerate(sentence):\n",
    "            if word in w2v_model.wv.vocab and word in tf:\n",
    "                weight = a/(a+tf[word])\n",
    "                w2v += w2v_model[word]*weight\n",
    "                j += 1\n",
    "        if j == 0:\n",
    "#             print('Invalid index: {}, sentence: {}'.format(i, sentence)) \n",
    "            invalid_indecis.append(i)\n",
    "            continue\n",
    "        s2v[i,:] = w2v/j\n",
    "    \n",
    "    return s2v, invalid_indecis\n",
    "\n",
    "def remove_pc(s2v, npc=1):\n",
    "    svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n",
    "    svd.fit(s2v)\n",
    "    u = svd.components_\n",
    "    s2v = s2v - s2v.dot(u.T).dot(u)\n",
    "    \n",
    "    return s2v    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "s2v, indecis = sentence_to_vector(sentences, model, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_final = comment.reset_index().drop(indecis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2v_final = np.delete(s2v, indecis, axis=0)\n",
    "s2v_final = remove_pc(s2v_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_final = np.array(comment_final['star'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((251842, 7), (251842, 200), (251842,))"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_final.shape, s2v_final.shape, star_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    80650\n",
      "3    63400\n",
      "5    56468\n",
      "2    27380\n",
      "1    23944\n",
      "Name: star, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b44b13c6d8>"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFaZJREFUeJzt3X+sX/V93/HnKzikkA5swoUxG9VU8ZIQshDwwFumKoXGGFLF/BFUs6pYiPVuyKzpNmlzNk1uIEhEmsaGlFJZwcGO2jiUNcJtTVwPQqdu/PDlxyBAkG8ghTvz47Y2P1ISmMl7f3w/rr/y+Zr7vRfjr5mfD+mrc8778znnfr5fWX7dc87ne0+qCkmS+r1v1AOQJB15DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOuaNegBzdfLJJ9fixYtHPQxJes948MEH/6qqxobp+54Nh8WLFzMxMTHqYUjSe0aSvxy2r5eVJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSx1DhkORfJXk8yfeTfCvJzyU5I8n9SXYm+XaSY1vfD7Ttyda+uO84X2r1p5Jc1Fdf0WqTSdYe6jcpSZqdGb8El2Qh8FvAmVX1kyS3AauAS4Abq2pzkt8DrgJubss9VfXhJKuArwK/luTMtt/Hgb8H/Pckf7/9mK8BnwWmgB1JtlTVE4f0nR5g8do/fTcPP7Qf3fC5UQ9BkjqGvaw0DzguyTzgeOB54ALg9ta+Ebi0ra9s27T2C5Ok1TdX1RtV9QwwCZzXXpNV9XRVvQlsbn0lSSMyYzhU1f8B/hPwLL1QeAV4EHi5qva2blPAwra+EHiu7bu39f9Qf/2AfQ5W70gynmQiycT09PQw70+SNAczhkOSBfR+kz+D3uWgDwIXD+ha+3Y5SNts691i1fqqWlpVS8fGhvrbUZKkORjmstKvAM9U1XRV/V/gj4B/DMxvl5kAFgG72voUcDpAaz8R2N1fP2Cfg9UlSSMyTDg8CyxLcny7d3Ah8ATwPeALrc9q4I62vqVt09rvrqpq9VVtNtMZwBLgAWAHsKTNfjqW3k3rLe/8rUmS5mrG2UpVdX+S24GHgL3Aw8B64E+BzUm+0mq3tF1uAb6ZZJLeGcOqdpzH20ynJ9px1lTVWwBJrgG2AccAG6rq8UP3FiVJszXU8xyqah2w7oDy0/RmGh3Y96fAZQc5zvXA9QPqW4Gtw4xFkvTu8xvSkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1zBgOST6S5JG+16tJfjvJSUm2J9nZlgta/yS5KclkkkeTnNN3rNWt/84kq/vq5yZ5rO1zU3scqSRpRGYMh6p6qqrOrqqzgXOB14HvAGuBu6pqCXBX2wa4mN7zoZcA48DNAElOovc0ufPpPUFu3b5AaX3G+/ZbcUjenSRpTmZ7WelC4IdV9ZfASmBjq28ELm3rK4FN1XMfMD/JacBFwPaq2l1Ve4DtwIrWdkJV3VtVBWzqO5YkaQRmGw6rgG+19VOr6nmAtjyl1RcCz/XtM9Vqb1efGlCXJI3I0OGQ5Fjg88AfztR1QK3mUB80hvEkE0kmpqenZxiGJGmuZnPmcDHwUFW92LZfbJeEaMuXWn0KOL1vv0XArhnqiwbUO6pqfVUtraqlY2Njsxi6JGk2ZhMOl7P/khLAFmDfjKPVwB199SvarKVlwCvtstM2YHmSBe1G9HJgW2t7LcmyNkvpir5jSZJGYN4wnZIcD3wW+Od95RuA25JcBTwLXNbqW4FLgEl6M5uuBKiq3UmuA3a0ftdW1e62fjVwK3AccGd7SZJGZKhwqKrXgQ8dUPtrerOXDuxbwJqDHGcDsGFAfQI4a5ixSJLefX5DWpLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqGOoP7+n/c79z4qhH0PM7r4x6BJIazxwkSR2GgySpw3CQJHUYDpKkjqHCIcn8JLcn+UGSJ5P8oyQnJdmeZGdbLmh9k+SmJJNJHk1yTt9xVrf+O5Os7qufm+Sxts9N7VnSkqQRGfbM4b8C362qjwKfBJ4E1gJ3VdUS4K62DXAxsKS9xoGbAZKcBKwDzgfOA9btC5TWZ7xvvxXv7G1Jkt6JGcMhyQnALwG3AFTVm1X1MrAS2Ni6bQQubesrgU3Vcx8wP8lpwEXA9qraXVV7gO3AitZ2QlXd254/vanvWJKkERjmzOEXgWngG0keTvL1JB8ETq2q5wHa8pTWfyHwXN/+U632dvWpAfWOJONJJpJMTE9PDzF0SdJcDBMO84BzgJur6lPA37D/EtIgg+4X1Bzq3WLV+qpaWlVLx8bG3n7UkqQ5GyYcpoCpqrq/bd9OLyxebJeEaMuX+vqf3rf/ImDXDPVFA+qSpBGZMRyq6gXguSQfaaULgSeALcC+GUergTva+hbgijZraRnwSrvstA1YnmRBuxG9HNjW2l5LsqzNUrqi71iSpBEY9m8r/Uvg95McCzwNXEkvWG5LchXwLHBZ67sVuASYBF5vfamq3UmuA3a0ftdW1e62fjVwK3AccGd7SZJGZKhwqKpHgKUDmi4c0LeANQc5zgZgw4D6BHDWMGORJL37/Ia0JKnDcJAkdRgOkqQOH/Yj9fnExk+MeggAPLb6sVEPQUc5zxwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUMVQ4JPlRkseSPJJkotVOSrI9yc62XNDqSXJTkskkjyY5p+84q1v/nUlW99XPbcefbPvmUL9RSdLwZnPm8MtVdXZV7Xtc6FrgrqpaAtzVtgEuBpa01zhwM/TCBFgHnA+cB6zbFyitz3jffivm/I4kSe/YO7mstBLY2NY3Apf21TdVz33A/CSnARcB26tqd1XtAbYDK1rbCVV1b3v+9Ka+Y0mSRmDYcCjgz5I8mGS81U6tqucB2vKUVl8IPNe371SrvV19akC9I8l4kokkE9PT00MOXZI0W8M+Ce7TVbUrySnA9iQ/eJu+g+4X1Bzq3WLVemA9wNKlSwf2kSS9c0OdOVTVrrZ8CfgOvXsGL7ZLQrTlS637FHB63+6LgF0z1BcNqEuSRmTGcEjywSR/Z986sBz4PrAF2DfjaDVwR1vfAlzRZi0tA15pl522AcuTLGg3opcD21rba0mWtVlKV/QdS5I0AsNcVjoV+E6bXToP+IOq+m6SHcBtSa4CngUua/23ApcAk8DrwJUAVbU7yXXAjtbv2qra3davBm4FjgPubC9J0ojMGA5V9TTwyQH1vwYuHFAvYM1BjrUB2DCgPgGcNcR4JUmHgd+QliR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpY+hwSHJMkoeT/EnbPiPJ/Ul2Jvl2kmNb/QNte7K1L+47xpda/akkF/XVV7TaZJK1h+7tSZLmYjZnDl8Enuzb/ipwY1UtAfYAV7X6VcCeqvowcGPrR5IzgVXAx4EVwO+2wDkG+BpwMXAmcHnrK0kakaHCIcki4HPA19t2gAuA21uXjcClbX1l26a1X9j6rwQ2V9UbVfUMvWdMn9dek1X1dFW9CWxufSVJIzLsmcN/Af4t8LO2/SHg5ara27angIVtfSHwHEBrf6X1/9v6AfscrC5JGpEZwyHJrwIvVdWD/eUBXWuGttnWB41lPMlEkonp6em3GbUk6Z0Y5szh08Dnk/yI3iWfC+idScxPMq/1WQTsautTwOkArf1EYHd//YB9DlbvqKr1VbW0qpaOjY0NMXRJ0lzMGA5V9aWqWlRVi+ndUL67qn4d+B7whdZtNXBHW9/Stmntd1dVtfqqNpvpDGAJ8ACwA1jSZj8d237GlkPy7iRJczJv5i4H9e+AzUm+AjwM3NLqtwDfTDJJ74xhFUBVPZ7kNuAJYC+wpqreAkhyDbANOAbYUFWPv4NxSZLeoVmFQ1XdA9zT1p+mN9PowD4/BS47yP7XA9cPqG8Fts5mLJKkd4/fkJYkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1zBgOSX4uyQNJ/neSx5N8udXPSHJ/kp1Jvt2e/0x7RvS3k0y29sV9x/pSqz+V5KK++opWm0yy9tC/TUnSbAxz5vAGcEFVfRI4G1iRZBnwVeDGqloC7AGuav2vAvZU1YeBG1s/kpxJ73nSHwdWAL+b5JgkxwBfAy4GzgQub30lSSMyYzhUz4/b5vvbq4ALgNtbfSNwaVtf2bZp7RcmSatvrqo3quoZYJLeM6jPAyar6umqehPY3PpKkkZkqHsO7Tf8R4CXgO3AD4GXq2pv6zIFLGzrC4HnAFr7K8CH+usH7HOw+qBxjCeZSDIxPT09zNAlSXMwVDhU1VtVdTawiN5v+h8b1K0tc5C22dYHjWN9VS2tqqVjY2MzD1ySNCezmq1UVS8D9wDLgPlJ5rWmRcCutj4FnA7Q2k8EdvfXD9jnYHVJ0ogMM1tpLMn8tn4c8CvAk8D3gC+0bquBO9r6lrZNa7+7qqrVV7XZTGcAS4AHgB3Akjb76Vh6N623HIo3J0mam3kzd+E0YGObVfQ+4Laq+pMkTwCbk3wFeBi4pfW/Bfhmkkl6ZwyrAKrq8SS3AU8Ae4E1VfUWQJJrgG3AMcCGqnr8kL1DSdKszRgOVfUo8KkB9afp3X84sP5T4LKDHOt64PoB9a3A1iHGK0k6DPyGtCSpY5jLSpKOQk9+dNCkxMPvYz94ctRDOCp55iBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh1+Ck6QZfO1f3D3qIQCw5vcuOGw/yzMHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpI5hniF9epLvJXkyyeNJvtjqJyXZnmRnWy5o9SS5KclkkkeTnNN3rNWt/84kq/vq5yZ5rO1zU5K8G29WkjScYc4c9gL/pqo+BiwD1iQ5E1gL3FVVS4C72jbAxcCS9hoHboZemADrgPPpPV503b5AaX3G+/Zb8c7fmiRprmYMh6p6vqoeauuvAU8CC4GVwMbWbSNwaVtfCWyqnvuA+UlOAy4CtlfV7qraA2wHVrS2E6rq3qoqYFPfsSRJIzCrew5JFgOfAu4HTq2q56EXIMAprdtC4Lm+3aZa7e3qUwPqg37+eJKJJBPT09OzGbokaRaGDockPw/8N+C3q+rVt+s6oFZzqHeLVeuramlVLR0bG5tpyJKkORoqHJK8n14w/H5V/VErv9guCdGWL7X6FHB63+6LgF0z1BcNqEuSRmSY2UoBbgGerKr/3Ne0Bdg342g1cEdf/Yo2a2kZ8Eq77LQNWJ5kQbsRvRzY1tpeS7Ks/awr+o4lSRqBYf4q66eB3wAeS/JIq/174AbgtiRXAc8Cl7W2rcAlwCTwOnAlQFXtTnIdsKP1u7aqdrf1q4FbgeOAO9tLkjQiM4ZDVf0Fg+8LAFw4oH8Baw5yrA3AhgH1CeCsmcYiSTo8/Ia0JKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdwzwmdEOSl5J8v692UpLtSXa25YJWT5KbkkwmeTTJOX37rG79dyZZ3Vc/N8ljbZ+b2qNCJUkjNMyZw63AigNqa4G7qmoJcFfbBrgYWNJe48DN0AsTYB1wPnAesG5foLQ+4337HfizJEmH2YzhUFX/A9h9QHklsLGtbwQu7atvqp77gPlJTgMuArZX1e6q2gNsB1a0thOq6t72eNFNfceSJI3IXO85nFpVzwO05SmtvhB4rq/fVKu9XX1qQF2SNEKH+ob0oPsFNYf64IMn40kmkkxMT0/PcYiSpJnMNRxebJeEaMuXWn0KOL2v3yJg1wz1RQPqA1XV+qpaWlVLx8bG5jh0SdJM5hoOW4B9M45WA3f01a9os5aWAa+0y07bgOVJFrQb0cuBba3ttSTL2iylK/qOJUkakXkzdUjyLeAzwMlJpujNOroBuC3JVcCzwGWt+1bgEmASeB24EqCqdie5DtjR+l1bVftucl9Nb0bUccCd7SVJGqEZw6GqLj9I04UD+haw5iDH2QBsGFCfAM6aaRySpMPHb0hLkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOo6YcEiyIslTSSaTrB31eCTpaHZEhEOSY4CvARcDZwKXJzlztKOSpKPXEREOwHnAZFU9XVVvApuBlSMekyQdtVJVox4DSb4ArKiqf9a2fwM4v6quOaDfODDeNj8CPHVYB9p1MvBXIx7DkcLPYj8/i/38LPY7Ej6LX6iqsWE6znu3RzKkDKh1Uquq1gPr3/3hDCfJRFUtHfU4jgR+Fvv5WeznZ7Hfe+2zOFIuK00Bp/dtLwJ2jWgsknTUO1LCYQewJMkZSY4FVgFbRjwmSTpqHRGXlapqb5JrgG3AMcCGqnp8xMMaxhFziesI4Gexn5/Ffn4W+72nPosj4oa0JOnIcqRcVpIkHUEMB0lSh+EgSeowHOYoyaZRj2GUkpyX5B+29TOT/Oskl4x6XJIOjSNittKRLsmB02oD/HKS+QBV9fnDP6rRSbKO3t/BmpdkO3A+cA+wNsmnqur6UY5vlJL8E3p/Dub7VfVnox7P4Zbko8BC4P6q+nFffUVVfXd0I9NsOVtpCEkeAp4Avk7vm9sBvkXv+xhU1Z+PbnSHX5LHgLOBDwAvAIuq6tUkx9H7T+EfjHSAh1GSB6rqvLb+m8Aa4DvAcuCPq+qGUY7vcEryW/Te/5P0/n18saruaG0PVdU5oxzfkSLJlVX1jVGPYyZeVhrOUuBB4D8Ar1TVPcBPqurPj7ZgaPZW1VtV9Trww6p6FaCqfgL8bLRDO+ze37c+Dny2qr5MLxx+fTRDGpnfBM6tqkuBzwD/MckXW9ugP5FztPryqAcwDC8rDaGqfgbcmOQP2/JFju7P7s0kx7dwOHdfMcmJHH3h8L4kC+j9opWqmgaoqr9Jsne0Qzvsjtl3KamqfpTkM8DtSX6Boywckjx6sCbg1MM5lrk6mv+Dm7WqmgIuS/I54NVRj2eEfqmq3oC/Dc593g+sHs2QRuZEemeVASrJ362qF5L8PEfZf4jAC0nOrqpHAKrqx0l+FdgAfGK0QzvsTgUuAvYcUA/wvw7/cGbPew7SuyDJ8cCpVfXMqMdyuCRZRO+S4wsD2j5dVf9zBMMaiSS3AN+oqr8Y0PYHVfVPRzCsWTEcJEkd3pCWJHUYDpKkDsNBktRhOEiSOv4flwTBRrPGnp4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "star_count = comment_final['star'].value_counts()\n",
    "print(star_count)\n",
    "star_count.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>151289</th>\n",
       "      <td>155902</td>\n",
       "      <td>154555</td>\n",
       "      <td>https://movie.douban.com/subject/1292625/</td>\n",
       "      <td>绿野仙踪 The Wizard of Oz</td>\n",
       "      <td>小时候的经典记忆，上次在课上重温，还是那么愉快。话说，其实原著的小画书也很好看。</td>\n",
       "      <td>4</td>\n",
       "      <td>[小时候, 的, 经典, 记忆, 上次, 在, 课上, 重温, 还是, 那么, 愉快, 话,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44866</th>\n",
       "      <td>45973</td>\n",
       "      <td>44626</td>\n",
       "      <td>https://movie.douban.com/subject/5323968/</td>\n",
       "      <td>环太平洋 Pacific Rim</td>\n",
       "      <td>怪兽这么大，地球住得下嘛？</td>\n",
       "      <td>4</td>\n",
       "      <td>[怪兽, 这么, 大, 地球, 住, 得, 下, 嘛]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74921</th>\n",
       "      <td>76945</td>\n",
       "      <td>75598</td>\n",
       "      <td>https://movie.douban.com/subject/1921583/</td>\n",
       "      <td>魔术师 L'illusionniste</td>\n",
       "      <td>虐心</td>\n",
       "      <td>4</td>\n",
       "      <td>[虐心]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226253</th>\n",
       "      <td>233307</td>\n",
       "      <td>231960</td>\n",
       "      <td>https://movie.douban.com/subject/1301212/</td>\n",
       "      <td>三匹之侍 三匹の侍</td>\n",
       "      <td>记得以前看过呢。btw，这条三区中文字幕可真差，给负分。</td>\n",
       "      <td>3</td>\n",
       "      <td>[记得, 以前, 看过, 呢, btw, 这条, 三区, 中文字幕, 可, 真差, 给, 负分]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228633</th>\n",
       "      <td>235785</td>\n",
       "      <td>234438</td>\n",
       "      <td>https://movie.douban.com/subject/1292055/</td>\n",
       "      <td>再见列宁 Good Bye Lenin!</td>\n",
       "      <td>主角长得很像卡卡</td>\n",
       "      <td>4</td>\n",
       "      <td>[主角, 长得, 很, 像, 卡卡]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184601</th>\n",
       "      <td>190405</td>\n",
       "      <td>189058</td>\n",
       "      <td>https://movie.douban.com/subject/1301445/</td>\n",
       "      <td>黑衣人 Men in Black</td>\n",
       "      <td>爆米花元素一应俱全，甚至还有些对人类自身的小调侃，段子和拌嘴很美咖，不错的科幻轻喜。</td>\n",
       "      <td>3</td>\n",
       "      <td>[爆米花, 元素, 一应俱全, 甚至, 还, 有些, 对, 人类, 自身, 的, 小, 调侃...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88918</th>\n",
       "      <td>91291</td>\n",
       "      <td>89944</td>\n",
       "      <td>https://movie.douban.com/subject/1293350/</td>\n",
       "      <td>两杆大烟枪 Lock, Stock and Two Smoking Barrels</td>\n",
       "      <td>电影好帅 OST好酷</td>\n",
       "      <td>5</td>\n",
       "      <td>[电影, 好帅, OST, 好酷]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69084</th>\n",
       "      <td>70895</td>\n",
       "      <td>69548</td>\n",
       "      <td>https://movie.douban.com/subject/24743712/</td>\n",
       "      <td>有种你爱我</td>\n",
       "      <td>挺烂的  话说江一燕皮肤真好 挺耐看</td>\n",
       "      <td>2</td>\n",
       "      <td>[挺烂, 的话, 说江, 一燕, 皮肤, 真好, 挺, 耐看]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77397</th>\n",
       "      <td>79479</td>\n",
       "      <td>78132</td>\n",
       "      <td>https://movie.douban.com/subject/10604893/</td>\n",
       "      <td>四大名捕大结局</td>\n",
       "      <td>甘道夫什么鬼</td>\n",
       "      <td>2</td>\n",
       "      <td>[甘道夫, 什么, 鬼]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37049</th>\n",
       "      <td>37961</td>\n",
       "      <td>36614</td>\n",
       "      <td>https://movie.douban.com/subject/20388224/</td>\n",
       "      <td>绝密跟踪 감시자들</td>\n",
       "      <td>翻拍得有模有样，不过略微装腔作势啊</td>\n",
       "      <td>3</td>\n",
       "      <td>[翻拍, 得, 有模有样, 不过, 略微, 装腔作势, 啊]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205679</th>\n",
       "      <td>212152</td>\n",
       "      <td>210805</td>\n",
       "      <td>https://movie.douban.com/subject/27018296/</td>\n",
       "      <td>之后 그 후</td>\n",
       "      <td>有意思有意思！！！金敏喜真是耐看型。</td>\n",
       "      <td>4</td>\n",
       "      <td>[有意思, 有意思, 金敏喜, 真是, 耐看, 型]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203125</th>\n",
       "      <td>209563</td>\n",
       "      <td>208216</td>\n",
       "      <td>https://movie.douban.com/subject/24873473/</td>\n",
       "      <td>疯狂72小时</td>\n",
       "      <td>无语~</td>\n",
       "      <td>2</td>\n",
       "      <td>[无, 语]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91997</th>\n",
       "      <td>94441</td>\n",
       "      <td>93094</td>\n",
       "      <td>https://movie.douban.com/subject/25717233/</td>\n",
       "      <td>心花路放</td>\n",
       "      <td>宁浩创作越来越敷衍了 比黄金大劫案还不如的水平 笑点倒是有 笑完什么都忘了</td>\n",
       "      <td>2</td>\n",
       "      <td>[宁浩, 创作, 越来越, 敷衍, 了, 比, 黄金, 大劫案, 还, 不如, 的, 水平,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243373</th>\n",
       "      <td>251168</td>\n",
       "      <td>249821</td>\n",
       "      <td>https://movie.douban.com/subject/1297710/</td>\n",
       "      <td>天若有情</td>\n",
       "      <td>我大概是2000年看的，当时就觉得华仔和吴倩莲的发型服饰各方面都——好土。</td>\n",
       "      <td>3</td>\n",
       "      <td>[我, 大概, 是, 2000, 年, 看, 的, 当时, 就, 觉得, 华仔, 和, 吴倩...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91757</th>\n",
       "      <td>94201</td>\n",
       "      <td>92854</td>\n",
       "      <td>https://movie.douban.com/subject/1304026/</td>\n",
       "      <td>我要复仇 복수는 나의 것</td>\n",
       "      <td>为了成为善良的“好”人，我们都活的过分辛苦。万物间的食物链，人世间的罪与罚，这些从来不会“越...</td>\n",
       "      <td>3</td>\n",
       "      <td>[为了, 成为, 善良, 的, 好人, 我们, 都, 活, 的, 过分, 辛苦, 万物, 间...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198130</th>\n",
       "      <td>204388</td>\n",
       "      <td>203041</td>\n",
       "      <td>https://movie.douban.com/subject/1302542/</td>\n",
       "      <td>血迷宫 Blood Simple</td>\n",
       "      <td>科恩出品，闷若磐石2……</td>\n",
       "      <td>3</td>\n",
       "      <td>[科恩, 出品, 闷, 若, 磐石, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32511</th>\n",
       "      <td>33315</td>\n",
       "      <td>31968</td>\n",
       "      <td>https://movie.douban.com/subject/3742360/</td>\n",
       "      <td>让子弹飞</td>\n",
       "      <td>根本不够格叫电影，就是一长篇相声。啥人物啊啥情感啊，啥都没。不过还可乐点。</td>\n",
       "      <td>2</td>\n",
       "      <td>[根本, 不够格, 叫, 电影, 就是, 一, 长篇, 相声, 啥, 人物, 啊, 啥, 情...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124894</th>\n",
       "      <td>128437</td>\n",
       "      <td>127090</td>\n",
       "      <td>https://movie.douban.com/subject/1298128/</td>\n",
       "      <td>西雅图未眠夜 Sleepless in Seattle</td>\n",
       "      <td>“我打算每天清晨起床，一整天呼吸，这样，过一阵子，我就可以不必再提醒自己‘起床’，‘呼吸’，...</td>\n",
       "      <td>4</td>\n",
       "      <td>[我, 打算, 每天, 清晨, 起床, 一整天, 唿吸, 这样, 过, 一阵子, 我, 就,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97774</th>\n",
       "      <td>100410</td>\n",
       "      <td>99063</td>\n",
       "      <td>https://movie.douban.com/subject/1305690/</td>\n",
       "      <td>阿飞正传 阿飛正傳</td>\n",
       "      <td>t1 阿飞正传.Days.Of.Being.Wild.1991.BD. TLF624  t3...</td>\n",
       "      <td>5</td>\n",
       "      <td>[t1, 阿飞, 正传, DaysOfBeingWild1991BDTLF624t3BD, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157039</th>\n",
       "      <td>161831</td>\n",
       "      <td>160484</td>\n",
       "      <td>https://movie.douban.com/subject/1306032/</td>\n",
       "      <td>缺席的人 The Man Who Wasn't There</td>\n",
       "      <td>摄影好，故事玄乎，松顿这回是真有范儿。科恩兄弟很善于借某件物品来表达一些深层的意味，这次是滚...</td>\n",
       "      <td>4</td>\n",
       "      <td>[摄影, 好, 故事, 玄乎, 松顿, 这回, 是, 真有, 范儿, 科恩, 兄弟, 很, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index      id                                        link  \\\n",
       "151289  155902  154555   https://movie.douban.com/subject/1292625/   \n",
       "44866    45973   44626   https://movie.douban.com/subject/5323968/   \n",
       "74921    76945   75598   https://movie.douban.com/subject/1921583/   \n",
       "226253  233307  231960   https://movie.douban.com/subject/1301212/   \n",
       "228633  235785  234438   https://movie.douban.com/subject/1292055/   \n",
       "184601  190405  189058   https://movie.douban.com/subject/1301445/   \n",
       "88918    91291   89944   https://movie.douban.com/subject/1293350/   \n",
       "69084    70895   69548  https://movie.douban.com/subject/24743712/   \n",
       "77397    79479   78132  https://movie.douban.com/subject/10604893/   \n",
       "37049    37961   36614  https://movie.douban.com/subject/20388224/   \n",
       "205679  212152  210805  https://movie.douban.com/subject/27018296/   \n",
       "203125  209563  208216  https://movie.douban.com/subject/24873473/   \n",
       "91997    94441   93094  https://movie.douban.com/subject/25717233/   \n",
       "243373  251168  249821   https://movie.douban.com/subject/1297710/   \n",
       "91757    94201   92854   https://movie.douban.com/subject/1304026/   \n",
       "198130  204388  203041   https://movie.douban.com/subject/1302542/   \n",
       "32511    33315   31968   https://movie.douban.com/subject/3742360/   \n",
       "124894  128437  127090   https://movie.douban.com/subject/1298128/   \n",
       "97774   100410   99063   https://movie.douban.com/subject/1305690/   \n",
       "157039  161831  160484   https://movie.douban.com/subject/1306032/   \n",
       "\n",
       "                                             name  \\\n",
       "151289                      绿野仙踪 The Wizard of Oz   \n",
       "44866                            环太平洋 Pacific Rim   \n",
       "74921                         魔术师 L'illusionniste   \n",
       "226253                                  三匹之侍 三匹の侍   \n",
       "228633                       再见列宁 Good Bye Lenin!   \n",
       "184601                           黑衣人 Men in Black   \n",
       "88918   两杆大烟枪 Lock, Stock and Two Smoking Barrels   \n",
       "69084                                       有种你爱我   \n",
       "77397                                     四大名捕大结局   \n",
       "37049                                   绝密跟踪 감시자들   \n",
       "205679                                     之后 그 후   \n",
       "203125                                     疯狂72小时   \n",
       "91997                                        心花路放   \n",
       "243373                                       天若有情   \n",
       "91757                               我要复仇 복수는 나의 것   \n",
       "198130                           血迷宫 Blood Simple   \n",
       "32511                                        让子弹飞   \n",
       "124894                西雅图未眠夜 Sleepless in Seattle   \n",
       "97774                                   阿飞正传 阿飛正傳   \n",
       "157039              缺席的人 The Man Who Wasn't There   \n",
       "\n",
       "                                                  comment  star  \\\n",
       "151289           小时候的经典记忆，上次在课上重温，还是那么愉快。话说，其实原著的小画书也很好看。     4   \n",
       "44866                                       怪兽这么大，地球住得下嘛？     4   \n",
       "74921                                                  虐心     4   \n",
       "226253                       记得以前看过呢。btw，这条三区中文字幕可真差，给负分。     3   \n",
       "228633                                           主角长得很像卡卡     4   \n",
       "184601         爆米花元素一应俱全，甚至还有些对人类自身的小调侃，段子和拌嘴很美咖，不错的科幻轻喜。     3   \n",
       "88918                                          电影好帅 OST好酷     5   \n",
       "69084                                  挺烂的  话说江一燕皮肤真好 挺耐看     2   \n",
       "77397                                              甘道夫什么鬼     2   \n",
       "37049                                   翻拍得有模有样，不过略微装腔作势啊     3   \n",
       "205679                                 有意思有意思！！！金敏喜真是耐看型。     4   \n",
       "203125                                                无语~     2   \n",
       "91997               宁浩创作越来越敷衍了 比黄金大劫案还不如的水平 笑点倒是有 笑完什么都忘了     2   \n",
       "243373              我大概是2000年看的，当时就觉得华仔和吴倩莲的发型服饰各方面都——好土。     3   \n",
       "91757   为了成为善良的“好”人，我们都活的过分辛苦。万物间的食物链，人世间的罪与罚，这些从来不会“越...     3   \n",
       "198130                                       科恩出品，闷若磐石2……     3   \n",
       "32511               根本不够格叫电影，就是一长篇相声。啥人物啊啥情感啊，啥都没。不过还可乐点。     2   \n",
       "124894  “我打算每天清晨起床，一整天呼吸，这样，过一阵子，我就可以不必再提醒自己‘起床’，‘呼吸’，...     4   \n",
       "97774   t1 阿飞正传.Days.Of.Being.Wild.1991.BD. TLF624  t3...     5   \n",
       "157039  摄影好，故事玄乎，松顿这回是真有范儿。科恩兄弟很善于借某件物品来表达一些深层的意味，这次是滚...     4   \n",
       "\n",
       "                                                    words  \n",
       "151289  [小时候, 的, 经典, 记忆, 上次, 在, 课上, 重温, 还是, 那么, 愉快, 话,...  \n",
       "44866                         [怪兽, 这么, 大, 地球, 住, 得, 下, 嘛]  \n",
       "74921                                                [虐心]  \n",
       "226253   [记得, 以前, 看过, 呢, btw, 这条, 三区, 中文字幕, 可, 真差, 给, 负分]  \n",
       "228633                                 [主角, 长得, 很, 像, 卡卡]  \n",
       "184601  [爆米花, 元素, 一应俱全, 甚至, 还, 有些, 对, 人类, 自身, 的, 小, 调侃...  \n",
       "88918                                   [电影, 好帅, OST, 好酷]  \n",
       "69084                     [挺烂, 的话, 说江, 一燕, 皮肤, 真好, 挺, 耐看]  \n",
       "77397                                        [甘道夫, 什么, 鬼]  \n",
       "37049                      [翻拍, 得, 有模有样, 不过, 略微, 装腔作势, 啊]  \n",
       "205679                         [有意思, 有意思, 金敏喜, 真是, 耐看, 型]  \n",
       "203125                                             [无, 语]  \n",
       "91997   [宁浩, 创作, 越来越, 敷衍, 了, 比, 黄金, 大劫案, 还, 不如, 的, 水平,...  \n",
       "243373  [我, 大概, 是, 2000, 年, 看, 的, 当时, 就, 觉得, 华仔, 和, 吴倩...  \n",
       "91757   [为了, 成为, 善良, 的, 好人, 我们, 都, 活, 的, 过分, 辛苦, 万物, 间...  \n",
       "198130                              [科恩, 出品, 闷, 若, 磐石, 2]  \n",
       "32511   [根本, 不够格, 叫, 电影, 就是, 一, 长篇, 相声, 啥, 人物, 啊, 啥, 情...  \n",
       "124894  [我, 打算, 每天, 清晨, 起床, 一整天, 唿吸, 这样, 过, 一阵子, 我, 就,...  \n",
       "97774   [t1, 阿飞, 正传, DaysOfBeingWild1991BDTLF624t3BD, ...  \n",
       "157039  [摄影, 好, 故事, 玄乎, 松顿, 这回, 是, 真有, 范儿, 科恩, 兄弟, 很, ...  "
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_final.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((251842, 200), (251842,))"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = s2v_final\n",
    "labels = star_final\n",
    "data.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (203991, 200) and (203991,)\n",
      "valid: (22666, 200) and (22666,)\n",
      "test: (25185, 200) and (25185,)\n"
     ]
    }
   ],
   "source": [
    "train_valid_data, test_data, train_valid_labels, test_labels = train_test_split(data, labels, test_size=0.1,random_state=42)\n",
    "train_data, valid_data, train_labels, valid_labels = train_test_split(train_valid_data, train_valid_labels, test_size=0.1,random_state=42)\n",
    "print('train: {} and {}'.format(train_data.shape, train_labels.shape))\n",
    "print('valid: {} and {}'.format(valid_data.shape, valid_labels.shape))    \n",
    "print('test: {} and {}'.format(test_data.shape, test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (203991, 200) and (203991, 5)\n",
      "valid: (22666, 200) and (22666, 5)\n",
      "test: (25185, 200) and (25185, 5)\n"
     ]
    }
   ],
   "source": [
    "def reformat(data, labels):\n",
    "    new_data = data\n",
    "    new_label = (labels[:, None] == np.arange(1, class_num+1)).astype(np.float64)\n",
    "    return new_data, new_label\n",
    "\n",
    "\n",
    "train_data, train_labels = reformat(train_data, train_labels)\n",
    "valid_data, valid_labels = reformat(valid_data, valid_labels)\n",
    "test_data, test_labels = reformat(test_data, test_labels)\n",
    "print('train: {} and {}'.format(train_data.shape, train_labels.shape))\n",
    "print('valid: {} and {}'.format(valid_data.shape, valid_labels.shape))    \n",
    "print('test: {} and {}'.format(test_data.shape, test_labels.shape))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 1, 4], dtype=int64)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(valid_labels[1:5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return 100.0*sum(np.argmax(predictions, 1)==np.argmax(labels, 1))/labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "feature_size = test_data.shape[1]\n",
    "h1 = 20\n",
    "beta = 0.01\n",
    "num_steps = 30001\n",
    "learn_rate = 0.01\n",
    "keep_prob = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initilazed\n",
      "Mini-batch train at step 0 with loss: 16.996919355000987\n",
      "Mini-batch train accuracy: 24.4, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 1000 with loss: 14.124478614428556\n",
      "Mini-batch train accuracy: 33.6, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 2000 with loss: 11.822902005283105\n",
      "Mini-batch train accuracy: 33.6, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 3000 with loss: 9.94844126650527\n",
      "Mini-batch train accuracy: 33.0, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 4000 with loss: 8.425868307109615\n",
      "Mini-batch train accuracy: 30.8, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 5000 with loss: 7.198398443079755\n",
      "Mini-batch train accuracy: 30.0, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 6000 with loss: 6.1503200744083735\n",
      "Mini-batch train accuracy: 33.8, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 7000 with loss: 5.334064010507232\n",
      "Mini-batch train accuracy: 29.6, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 8000 with loss: 4.6246730869491195\n",
      "Mini-batch train accuracy: 32.2, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 9000 with loss: 4.0541141782715915\n",
      "Mini-batch train accuracy: 32.4, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 10000 with loss: 3.592394768557516\n",
      "Mini-batch train accuracy: 30.2, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 11000 with loss: 3.217460065335955\n",
      "Mini-batch train accuracy: 34.8, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 12000 with loss: 2.925569930826214\n",
      "Mini-batch train accuracy: 32.0, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 13000 with loss: 2.6431582440816026\n",
      "Mini-batch train accuracy: 32.0, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 14000 with loss: 2.4373463871257774\n",
      "Mini-batch train accuracy: 36.4, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 15000 with loss: 2.283210040538438\n",
      "Mini-batch train accuracy: 33.8, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 16000 with loss: 2.131229853273968\n",
      "Mini-batch train accuracy: 32.2, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 17000 with loss: 2.0303915938636927\n",
      "Mini-batch train accuracy: 30.2, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 18000 with loss: 1.9498464349516915\n",
      "Mini-batch train accuracy: 30.8, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 19000 with loss: 1.8796526324515481\n",
      "Mini-batch train accuracy: 28.4, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 20000 with loss: 1.7893681651727573\n",
      "Mini-batch train accuracy: 33.0, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 21000 with loss: 1.7666964452705551\n",
      "Mini-batch train accuracy: 28.2, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 22000 with loss: 1.698106692772277\n",
      "Mini-batch train accuracy: 32.0, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 23000 with loss: 1.6537933591394425\n",
      "Mini-batch train accuracy: 34.4, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 24000 with loss: 1.617869153732934\n",
      "Mini-batch train accuracy: 34.6, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 25000 with loss: 1.617871664191424\n",
      "Mini-batch train accuracy: 31.2, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 26000 with loss: 1.5926890948479044\n",
      "Mini-batch train accuracy: 30.2, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 27000 with loss: 1.5974647189505176\n",
      "Mini-batch train accuracy: 29.0, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 28000 with loss: 1.5545378810447081\n",
      "Mini-batch train accuracy: 33.8, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 29000 with loss: 1.5823923289682809\n",
      "Mini-batch train accuracy: 31.0, valid accuracy: 31.81417100502956\n",
      "Mini-batch train at step 30000 with loss: 1.5428756366657208\n",
      "Mini-batch train accuracy: 34.6, valid accuracy: 31.81417100502956\n",
      "Mini-batch test accuarcy: 32.189795513202306\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    # input, only train is with SGD\n",
    "    tf_train_data = tf.placeholder(tf.float64, shape=(batch_size, feature_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float64, shape=(batch_size, class_num))\n",
    "    tf_valid_data = tf.constant(valid_data)\n",
    "    tf_test_data = tf.constant(test_data)\n",
    "    \n",
    "    # variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([feature_size, h1], dtype=np.float64))\n",
    "    biases1 = tf.Variable(tf.zeros(h1, dtype=np.float64))  \n",
    "    weights2 = tf.Variable(tf.truncated_normal([h1, class_num], dtype=np.float64))\n",
    "    biases2 = tf.Variable(tf.zeros(class_num, dtype=np.float64))  \n",
    "    \n",
    "    # train compution   \n",
    "    logits_1 = tf.matmul(tf_train_data, weights1) + biases1\n",
    "    relu_layer = tf.nn.relu(logits_1)\n",
    "    dropout_layer = tf.nn.dropout(relu_layer, keep_prob=keep_prob)\n",
    "    logits_2 = tf.matmul(dropout_layer, weights2) + biases2\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits_2))\n",
    "    regularization = beta*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(biases1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(biases2))\n",
    "    loss = tf.reduce_mean(loss + regularization)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss)\n",
    "    \n",
    "    # prediction\n",
    "    train_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    logits_1 = tf.matmul(tf_valid_data, weights1) + biases1\n",
    "    relu_layer = tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights2) + biases2\n",
    "    valid_prediction = tf.nn.softmax(logits_2)\n",
    "\n",
    "    logits_1 = tf.matmul(tf_test_data, weights1) + biases1\n",
    "    relu_layer = tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights2) + biases2\n",
    "    test_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "with tf.Session(graph=g) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initilazed')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step*batch_size)%(train_labels.shape[0]-batch_size)\n",
    "        # generate a mini-batch\n",
    "        batch_data = train_data[offset:(offset+batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size), :]\n",
    "        \n",
    "        # dict to feed mini-batch\n",
    "        feed_dict = {tf_train_data: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        # logging\n",
    "        if step % 1000 == 0:\n",
    "            print('Mini-batch train at step {} with loss: {}'.format(step, l))\n",
    "            print('Mini-batch train accuracy: {}, valid accuracy: {}'.format(\n",
    "                 accuracy(predictions, batch_labels),\n",
    "                 accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print('Mini-batch test accuarcy: {}'.format(accuracy(test_prediction.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    '''Model function of CNN'''\n",
    "    \n",
    "    # Input layer\n",
    "    input_layer = tf.reshape(features['x'], [-1, 10, 20, 1])\n",
    "    print(input_layer.shape)\n",
    "    \n",
    "    # Convolutional layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs = input_layer, \n",
    "        filters=32,\n",
    "        kernel_size=[5, 5], \n",
    "        padding='same', \n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    # Pooling layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    # Convolutional layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1, \n",
    "        filters=64, \n",
    "        kernel_size=[5, 5], \n",
    "        padding='same', \n",
    "        activation=tf.nn.relu)\n",
    "      \n",
    "    # Pooling layer #1\n",
    "#     pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    # Dense layer #1\n",
    "    pool2_flat = tf.reshape(conv2, [-1, 5*10*64])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=0.4, \n",
    "                                training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    #Logits layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=class_num)\n",
    "    \n",
    "    predictions = {\n",
    "        'classes': tf.argmax(input=logits, axis=1),\n",
    "        'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    # Loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits))\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "        \n",
    "    eval_metric_ops = {\n",
    "        'accuracy': tf.metrics.accuracy(labels=tf.argmax(input=labels, axis=1),\n",
    "                                        predictions=predictions['classes'])\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,loss=loss,\n",
    "                                      eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_num_ps_replicas': 0, '_is_chief': True, '_save_summary_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002B3D129AB00>, '_session_config': None, '_task_id': 0, '_global_id_in_cluster': 0, '_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': None, '_train_distribute': None, '_device_fn': None, '_log_step_count_steps': 100, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_master': '', '_save_checkpoints_steps': None, '_evaluation_master': '', '_num_worker_replicas': 1, '_model_dir': 'C:\\\\Users\\\\7153678\\\\Desktop\\\\AI\\\\src\\\\nlp\\\\homework\\\\lesson10', '_save_checkpoints_secs': 600, '_service': None}\n"
     ]
    }
   ],
   "source": [
    "# Create the Estimator\n",
    "data_root = r'C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\homework\\lesson10'\n",
    "mnist_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": train_data},\n",
    "    y=train_labels,\n",
    "    batch_size=100,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "(100, 10, 20, 1)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\homework\\lesson10\\model.ckpt.\n",
      "INFO:tensorflow:probabilities = [[0.20000001 0.19999998 0.20000001 0.2        0.2       ]\n",
      " [0.20000001 0.19999999 0.20000001 0.2        0.2       ]\n",
      " [0.2        0.19999999 0.2        0.2        0.2       ]\n",
      " [0.20000001 0.19999999 0.20000002 0.19999999 0.19999999]\n",
      " [0.2        0.2        0.2        0.19999999 0.20000001]\n",
      " [0.2        0.2        0.20000001 0.2        0.19999999]\n",
      " [0.20000001 0.2        0.2        0.2        0.19999999]\n",
      " [0.20000001 0.19999998 0.20000002 0.19999999 0.20000001]\n",
      " [0.20000001 0.2        0.2        0.19999999 0.2       ]\n",
      " [0.2        0.19999999 0.20000001 0.2        0.2       ]\n",
      " [0.2        0.19999999 0.20000001 0.2        0.2       ]\n",
      " [0.20000001 0.19999999 0.20000001 0.20000001 0.19999999]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.20000004 0.19999997 0.20000001 0.2        0.19999999]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.20000005 0.19999998 0.20000002 0.19999995 0.20000001]\n",
      " [0.20000001 0.2        0.2        0.19999999 0.2       ]\n",
      " [0.20000003 0.19999999 0.19999998 0.20000001 0.19999999]\n",
      " [0.20000004 0.19999994 0.20000003 0.19999998 0.2       ]\n",
      " [0.2        0.2        0.20000001 0.2        0.19999999]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.20000003 0.19999998 0.2        0.19999998 0.20000002]\n",
      " [0.20000001 0.2        0.2        0.19999999 0.2       ]\n",
      " [0.20000001 0.19999999 0.2        0.2        0.2       ]\n",
      " [0.20000001 0.20000001 0.2        0.19999999 0.19999999]\n",
      " [0.20000002 0.2        0.20000003 0.19999995 0.2       ]\n",
      " [0.2        0.19999999 0.20000001 0.20000001 0.19999999]\n",
      " [0.20000001 0.2        0.2        0.2        0.2       ]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.2        0.2        0.20000001 0.2        0.2       ]\n",
      " [0.20000001 0.19999998 0.2        0.2        0.20000001]\n",
      " [0.20000001 0.2        0.2        0.19999999 0.2       ]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.20000001 0.19999999 0.20000001 0.19999999 0.2       ]\n",
      " [0.20000001 0.1999999  0.20000007 0.20000002 0.2       ]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.20000002 0.19999999 0.19999998 0.2        0.20000001]\n",
      " [0.20000001 0.19999999 0.20000001 0.19999999 0.2       ]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.2        0.19999999 0.2        0.2        0.20000001]\n",
      " [0.20000001 0.19999999 0.20000001 0.2        0.19999999]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.20000001 0.19999999 0.2        0.2        0.20000001]\n",
      " [0.20000001 0.20000001 0.20000002 0.19999997 0.2       ]\n",
      " [0.2        0.2        0.19999999 0.2        0.2       ]\n",
      " [0.20000001 0.19999999 0.2        0.19999999 0.2       ]\n",
      " [0.20000001 0.19999999 0.19999999 0.2        0.2       ]\n",
      " [0.20000001 0.2        0.2        0.19999999 0.2       ]\n",
      " [0.2        0.2        0.20000001 0.2        0.2       ]\n",
      " [0.20000001 0.19999993 0.2000001  0.19999998 0.19999998]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.2        0.19999999 0.2        0.2        0.2       ]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.20000006 0.1999999  0.2000001  0.19999989 0.20000005]\n",
      " [0.20000001 0.19999999 0.20000002 0.19999999 0.19999999]\n",
      " [0.20000001 0.19999999 0.20000001 0.19999999 0.2       ]\n",
      " [0.2        0.19999999 0.20000001 0.2        0.2       ]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.20000001 0.19999999 0.20000002 0.19999999 0.19999999]\n",
      " [0.2        0.19999999 0.2        0.2        0.2       ]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.2        0.19999999 0.20000001 0.2        0.2       ]\n",
      " [0.2        0.19999999 0.20000001 0.2        0.2       ]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.19999999 0.19999996 0.20000006 0.19999997 0.20000003]\n",
      " [0.20000001 0.2        0.2        0.19999999 0.2       ]\n",
      " [0.20000001 0.19999997 0.20000002 0.20000001 0.2       ]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.2        0.2        0.20000001 0.19999999 0.2       ]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.20000002 0.19999999 0.20000001 0.2        0.19999999]\n",
      " [0.2        0.2        0.2        0.19999999 0.2       ]\n",
      " [0.20000001 0.2        0.20000001 0.19999998 0.20000001]\n",
      " [0.20000001 0.19999999 0.20000001 0.19999999 0.2       ]\n",
      " [0.20000007 0.19999995 0.20000005 0.19999997 0.19999997]\n",
      " [0.20000001 0.19999998 0.2        0.2        0.20000001]\n",
      " [0.20000002 0.19999999 0.20000001 0.19999999 0.19999999]\n",
      " [0.2        0.19999999 0.20000001 0.2        0.20000001]\n",
      " [0.20000002 0.19999999 0.20000001 0.19999998 0.20000001]\n",
      " [0.20000002 0.19999997 0.20000002 0.20000001 0.19999998]\n",
      " [0.20000001 0.19999999 0.20000001 0.2        0.2       ]\n",
      " [0.19999999 0.20000001 0.2        0.19999999 0.2       ]\n",
      " [0.20000005 0.1999999  0.20000013 0.19999995 0.19999997]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.20000003 0.19999998 0.2        0.19999998 0.2       ]\n",
      " [0.20000001 0.19999998 0.19999999 0.2        0.20000002]\n",
      " [0.20000002 0.19999999 0.2        0.19999999 0.20000001]\n",
      " [0.20000001 0.19999999 0.20000003 0.19999999 0.19999998]\n",
      " [0.20000001 0.2        0.20000003 0.19999997 0.20000001]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.2        0.19999999 0.20000001 0.2        0.2       ]\n",
      " [0.20000001 0.2        0.2        0.19999999 0.2       ]\n",
      " [0.20000001 0.2        0.2        0.19999999 0.2       ]\n",
      " [0.20000013 0.19999997 0.20000002 0.19999995 0.19999993]\n",
      " [0.2        0.2        0.2        0.2        0.2       ]]\n",
      "INFO:tensorflow:loss = 1.6094379130156795, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\homework\\lesson10\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.6094379130156795.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x2b3d129a940>"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train one step and display the probabilties\n",
    "mnist_classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=1,\n",
    "    hooks=[logging_hook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "(?, 10, 20, 1)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-09-23-12:19:14\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\homework\\lesson10\\model.ckpt-1\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-09-23-12:19:45\n",
      "INFO:tensorflow:Saving dict for global step 1: accuracy = 0.25229302, global_step = 1, loss = 1.6073837\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1: C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\homework\\lesson10\\model.ckpt-1\n",
      "{'accuracy': 0.25229302, 'global_step': 1, 'loss': 1.6073837}\n"
     ]
    }
   ],
   "source": [
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": test_data},\n",
    "    y=test_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "test_results = mnist_classifier.evaluate(input_fn=test_input_fn)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
