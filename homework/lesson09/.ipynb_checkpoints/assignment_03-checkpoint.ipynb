{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Imporve NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = r'C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\data\\mnist'\n",
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "image_size = 28\n",
    "class_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (200000, 28, 28) and (200000,)\n",
      "valid: (10000, 28, 28) and (10000,)\n",
      "test: (10000, 28, 28) and (10000,)\n"
     ]
    }
   ],
   "source": [
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_data = save['train_data']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_data = save['valid_data']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_data = save['test_data']\n",
    "    test_labels = save['test_labels']\n",
    "    del save\n",
    "    \n",
    "print('train: {} and {}'.format(train_data.shape, train_labels.shape))\n",
    "print('valid: {} and {}'.format(valid_data.shape, valid_labels.shape))    \n",
    "print('test: {} and {}'.format(test_data.shape, test_labels.shape)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (200000, 784) and (200000, 10)\n",
      "valid: (10000, 784) and (10000, 10)\n",
      "test: (10000, 784) and (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "def reformat(data, labels):\n",
    "    new_data = data.reshape(-1, image_size**2)\n",
    "    new_label = (labels[:, None] == np.arange(class_num)).astype(np.float32)\n",
    "    return new_data, new_label\n",
    "\n",
    "train_data, train_labels = reformat(train_data, train_labels)\n",
    "valid_data, valid_labels = reformat(valid_data, valid_labels)\n",
    "test_data, test_labels = reformat(test_data, test_labels)\n",
    "print('train: {} and {}'.format(train_data.shape, train_labels.shape))\n",
    "print('valid: {} and {}'.format(valid_data.shape, valid_labels.shape))    \n",
    "print('test: {} and {}'.format(test_data.shape, test_labels.shape))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return 100.0*sum(np.argmax(predictions, 1)==np.argmax(labels, 1))/labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "graph = tf.Graph()\n",
    "h1 = 1024\n",
    "beta = 0.01\n",
    "num_steps = 20001\n",
    "learn_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # input, only train is with SGD\n",
    "    tf_train_data = tf.placeholder(tf.float32, shape=(batch_size, image_size**2))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, class_num))\n",
    "    tf_valid_data = tf.constant(valid_data)\n",
    "    tf_test_data = tf.constant(test_data)\n",
    "    \n",
    "    # variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size**2, h1]))\n",
    "    biases1 = tf.Variable(tf.zeros(h1))  \n",
    "    weights2 = tf.Variable(tf.truncated_normal([h1, class_num]))\n",
    "    biases2 = tf.Variable(tf.zeros(class_num))  \n",
    "    \n",
    "    # train compution   \n",
    "    logits_1 = tf.matmul(tf_train_data, weights1) + biases1\n",
    "    relu_layer = tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits_2))\n",
    "    regularization = beta*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(biases1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(biases2))\n",
    "    loss = tf.reduce_mean(loss + regularization)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss)\n",
    "    \n",
    "    # prediction\n",
    "    train_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    logits_1 = tf.matmul(tf_valid_data, weights1) + biases1\n",
    "    relu_layer = tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights2) + biases2\n",
    "    valid_prediction = tf.nn.softmax(logits_2)\n",
    "\n",
    "    logits_1 = tf.matmul(tf_test_data, weights1) + biases1\n",
    "    relu_layer = tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights2) + biases2\n",
    "    test_prediction = tf.nn.softmax(logits_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initilazed\n",
      "Mini-batch train at step 0 with loss: 3490.550048828125\n",
      "Mini-batch train accuracy: 9.375, valid accuracy: 27.53\n",
      "Mini-batch train at step 500 with loss: 21.42230987548828\n",
      "Mini-batch train accuracy: 75.78125, valid accuracy: 84.06\n",
      "Mini-batch train at step 1000 with loss: 0.923163652420044\n",
      "Mini-batch train accuracy: 84.375, valid accuracy: 83.82\n",
      "Mini-batch train at step 1500 with loss: 0.8249024152755737\n",
      "Mini-batch train accuracy: 82.8125, valid accuracy: 83.92\n",
      "Mini-batch train at step 2000 with loss: 0.5819976329803467\n",
      "Mini-batch train accuracy: 87.5, valid accuracy: 82.47\n",
      "Mini-batch train at step 2500 with loss: 0.7443509101867676\n",
      "Mini-batch train accuracy: 82.03125, valid accuracy: 82.41\n",
      "Mini-batch train at step 3000 with loss: 0.8527445793151855\n",
      "Mini-batch train accuracy: 78.90625, valid accuracy: 83.75\n",
      "Mini-batch train at step 3500 with loss: 0.7074048519134521\n",
      "Mini-batch train accuracy: 82.8125, valid accuracy: 83.69\n",
      "Mini-batch train at step 4000 with loss: 0.712395191192627\n",
      "Mini-batch train accuracy: 84.375, valid accuracy: 83.0\n",
      "Mini-batch train at step 4500 with loss: 0.7045676708221436\n",
      "Mini-batch train accuracy: 84.375, valid accuracy: 83.85\n",
      "Mini-batch train at step 5000 with loss: 0.8215199708938599\n",
      "Mini-batch train accuracy: 84.375, valid accuracy: 83.67\n",
      "Mini-batch train at step 5500 with loss: 0.678775429725647\n",
      "Mini-batch train accuracy: 88.28125, valid accuracy: 83.72\n",
      "Mini-batch train at step 6000 with loss: 0.6829909086227417\n",
      "Mini-batch train accuracy: 85.15625, valid accuracy: 83.67\n",
      "Mini-batch train at step 6500 with loss: 0.6299096941947937\n",
      "Mini-batch train accuracy: 85.15625, valid accuracy: 83.93\n",
      "Mini-batch train at step 7000 with loss: 0.8449565172195435\n",
      "Mini-batch train accuracy: 79.6875, valid accuracy: 83.29\n",
      "Mini-batch train at step 7500 with loss: 0.7067694664001465\n",
      "Mini-batch train accuracy: 85.9375, valid accuracy: 83.56\n",
      "Mini-batch train at step 8000 with loss: 0.7204272747039795\n",
      "Mini-batch train accuracy: 84.375, valid accuracy: 84.19\n",
      "Mini-batch train at step 8500 with loss: 0.8090397715568542\n",
      "Mini-batch train accuracy: 79.6875, valid accuracy: 84.39\n",
      "Mini-batch train at step 9000 with loss: 0.6484295129776001\n",
      "Mini-batch train accuracy: 84.375, valid accuracy: 83.69\n",
      "Mini-batch train at step 9500 with loss: 0.7346052527427673\n",
      "Mini-batch train accuracy: 82.03125, valid accuracy: 83.57\n",
      "Mini-batch train at step 10000 with loss: 0.8269962072372437\n",
      "Mini-batch train accuracy: 77.34375, valid accuracy: 84.26\n",
      "Mini-batch train at step 10500 with loss: 0.7868776321411133\n",
      "Mini-batch train accuracy: 85.9375, valid accuracy: 84.01\n",
      "Mini-batch train at step 11000 with loss: 0.7149748802185059\n",
      "Mini-batch train accuracy: 82.8125, valid accuracy: 84.33\n",
      "Mini-batch train at step 11500 with loss: 0.7395022511482239\n",
      "Mini-batch train accuracy: 85.15625, valid accuracy: 84.12\n",
      "Mini-batch train at step 12000 with loss: 0.632616400718689\n",
      "Mini-batch train accuracy: 87.5, valid accuracy: 84.19\n",
      "Mini-batch train at step 12500 with loss: 0.6565279960632324\n",
      "Mini-batch train accuracy: 85.15625, valid accuracy: 82.97\n",
      "Mini-batch train at step 13000 with loss: 0.7075578570365906\n",
      "Mini-batch train accuracy: 85.15625, valid accuracy: 83.85\n",
      "Mini-batch train at step 13500 with loss: 0.7177146077156067\n",
      "Mini-batch train accuracy: 82.03125, valid accuracy: 83.3\n",
      "Mini-batch train at step 14000 with loss: 0.7480801343917847\n",
      "Mini-batch train accuracy: 84.375, valid accuracy: 83.78\n",
      "Mini-batch train at step 14500 with loss: 0.6197552680969238\n",
      "Mini-batch train accuracy: 90.625, valid accuracy: 83.91\n",
      "Mini-batch train at step 15000 with loss: 0.7756852507591248\n",
      "Mini-batch train accuracy: 82.03125, valid accuracy: 83.98\n",
      "Mini-batch train at step 15500 with loss: 0.7869161367416382\n",
      "Mini-batch train accuracy: 83.59375, valid accuracy: 84.35\n",
      "Mini-batch train at step 16000 with loss: 0.8255676627159119\n",
      "Mini-batch train accuracy: 83.59375, valid accuracy: 83.42\n",
      "Mini-batch train at step 16500 with loss: 0.8273897767066956\n",
      "Mini-batch train accuracy: 82.8125, valid accuracy: 84.15\n",
      "Mini-batch train at step 17000 with loss: 0.9045417308807373\n",
      "Mini-batch train accuracy: 79.6875, valid accuracy: 84.26\n",
      "Mini-batch train at step 17500 with loss: 0.7875783443450928\n",
      "Mini-batch train accuracy: 82.8125, valid accuracy: 84.57\n",
      "Mini-batch train at step 18000 with loss: 0.9319275617599487\n",
      "Mini-batch train accuracy: 78.125, valid accuracy: 83.44\n",
      "Mini-batch train at step 18500 with loss: 0.7403297424316406\n",
      "Mini-batch train accuracy: 83.59375, valid accuracy: 83.36\n",
      "Mini-batch train at step 19000 with loss: 0.8079392313957214\n",
      "Mini-batch train accuracy: 82.03125, valid accuracy: 82.53\n",
      "Mini-batch train at step 19500 with loss: 0.6900070905685425\n",
      "Mini-batch train accuracy: 85.15625, valid accuracy: 84.51\n",
      "Mini-batch train at step 20000 with loss: 0.6205147504806519\n",
      "Mini-batch train accuracy: 85.9375, valid accuracy: 83.96\n",
      "Mini-batch test accuarcy: 89.74\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initilazed')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step*batch_size)%(train_labels.shape[0]-batch_size)\n",
    "        # generate a mini-batch\n",
    "        batch_data = train_data[offset:(offset+batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size), :]\n",
    "        \n",
    "        # dict to feed mini-batch\n",
    "        feed_dict = {tf_train_data: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        # logging\n",
    "        if step % 500 == 0:\n",
    "            print('Mini-batch train at step {} with loss: {}'.format(step, l))\n",
    "            print('Mini-batch train accuracy: {}, valid accuracy: {}'.format(\n",
    "                 accuracy(predictions, batch_labels),\n",
    "                 accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print('Mini-batch test accuarcy: {}'.format(accuracy(test_prediction.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "graph = tf.Graph()\n",
    "h1 = 1024\n",
    "beta = 0.01\n",
    "num_steps = 20001\n",
    "learn_rate = 0.5\n",
    "\n",
    "train_data2 = train_data[:1000, :]\n",
    "train_labels2 = train_labels[:1000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initilazed\n",
      "Mini-batch train at step 0 with loss: 3439.205078125\n",
      "Mini-batch train accuracy: 8.59375, valid accuracy: 28.96\n",
      "Mini-batch train at step 500 with loss: 21.05567741394043\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.36\n",
      "Mini-batch train at step 1000 with loss: 0.5125932693481445\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.82\n",
      "Mini-batch train at step 1500 with loss: 0.35965293645858765\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.19\n",
      "Mini-batch train at step 2000 with loss: 0.3735182583332062\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.15\n",
      "Mini-batch train at step 2500 with loss: 0.43856197595596313\n",
      "Mini-batch train accuracy: 97.65625, valid accuracy: 80.09\n",
      "Mini-batch train at step 3000 with loss: 0.35190993547439575\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.86\n",
      "Mini-batch train at step 3500 with loss: 0.3493117094039917\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.83\n",
      "Mini-batch train at step 4000 with loss: 0.3305906653404236\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.2\n",
      "Mini-batch train at step 4500 with loss: 0.36666667461395264\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.73\n",
      "Mini-batch train at step 5000 with loss: 0.38387587666511536\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.57\n",
      "Mini-batch train at step 5500 with loss: 0.3400357663631439\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.71\n",
      "Mini-batch train at step 6000 with loss: 0.35001617670059204\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.61\n",
      "Mini-batch train at step 6500 with loss: 0.3388790786266327\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.23\n",
      "Mini-batch train at step 7000 with loss: 0.35684090852737427\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.76\n",
      "Mini-batch train at step 7500 with loss: 0.35784822702407837\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.6\n",
      "Mini-batch train at step 8000 with loss: 0.3403271734714508\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.0\n",
      "Mini-batch train at step 8500 with loss: 0.35067230463027954\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.66\n",
      "Mini-batch train at step 9000 with loss: 0.3232257068157196\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.84\n",
      "Mini-batch train at step 9500 with loss: 0.3621949553489685\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.95\n",
      "Mini-batch train at step 10000 with loss: 0.3515438139438629\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.62\n",
      "Mini-batch train at step 10500 with loss: 0.3549546003341675\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.31\n",
      "Mini-batch train at step 11000 with loss: 0.35120558738708496\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.64\n",
      "Mini-batch train at step 11500 with loss: 0.3455505967140198\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.14\n",
      "Mini-batch train at step 12000 with loss: 0.3685467839241028\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.06\n",
      "Mini-batch train at step 12500 with loss: 0.3526706397533417\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.66\n",
      "Mini-batch train at step 13000 with loss: 0.3528478145599365\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.95\n",
      "Mini-batch train at step 13500 with loss: 0.3568929433822632\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.03\n",
      "Mini-batch train at step 14000 with loss: 0.36520639061927795\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.21\n",
      "Mini-batch train at step 14500 with loss: 0.361222505569458\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.91\n",
      "Mini-batch train at step 15000 with loss: 0.34562498331069946\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.51\n",
      "Mini-batch train at step 15500 with loss: 0.34823593497276306\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.78\n",
      "Mini-batch train at step 16000 with loss: 0.3559063673019409\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.74\n",
      "Mini-batch train at step 16500 with loss: 0.40669873356819153\n",
      "Mini-batch train accuracy: 99.21875, valid accuracy: 81.13\n",
      "Mini-batch train at step 17000 with loss: 0.3509117364883423\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.0\n",
      "Mini-batch train at step 17500 with loss: 0.34221351146698\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.67\n",
      "Mini-batch train at step 18000 with loss: 0.33764347434043884\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.99\n",
      "Mini-batch train at step 18500 with loss: 0.3527904748916626\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.9\n",
      "Mini-batch train at step 19000 with loss: 0.5549172163009644\n",
      "Mini-batch train accuracy: 94.53125, valid accuracy: 79.57\n",
      "Mini-batch train at step 19500 with loss: 0.34929341077804565\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.1\n",
      "Mini-batch train at step 20000 with loss: 0.3418259620666504\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.65\n",
      "Mini-batch test accuarcy: 87.51\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    # input, only train is with SGD\n",
    "    tf_train_data = tf.placeholder(tf.float32, shape=(batch_size, image_size**2))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, class_num))\n",
    "    tf_valid_data = tf.constant(valid_data)\n",
    "    tf_test_data = tf.constant(test_data)\n",
    "    \n",
    "    # variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size**2, h1]))\n",
    "    biases1 = tf.Variable(tf.zeros(h1))  \n",
    "    weights2 = tf.Variable(tf.truncated_normal([h1, class_num]))\n",
    "    biases2 = tf.Variable(tf.zeros(class_num))  \n",
    "    \n",
    "    # train compution   \n",
    "    logits_1 = tf.matmul(tf_train_data, weights1) + biases1\n",
    "    relu_layer = tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits_2))\n",
    "    regularization = beta*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(biases1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(biases2))\n",
    "    loss = tf.reduce_mean(loss + regularization)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss)\n",
    "    \n",
    "    # prediction\n",
    "    train_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    logits_1 = tf.matmul(tf_valid_data, weights1) + biases1\n",
    "    relu_layer = tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights2) + biases2\n",
    "    valid_prediction = tf.nn.softmax(logits_2)\n",
    "\n",
    "    logits_1 = tf.matmul(tf_test_data, weights1) + biases1\n",
    "    relu_layer = tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights2) + biases2\n",
    "    test_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "with tf.Session(graph=g) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initilazed')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step*batch_size)%(train_labels2.shape[0]-batch_size)\n",
    "        # generate a mini-batch\n",
    "        batch_data = train_data2[offset:(offset+batch_size), :]\n",
    "        batch_labels = train_labels2[offset:(offset+batch_size), :]\n",
    "        \n",
    "        # dict to feed mini-batch\n",
    "        feed_dict = {tf_train_data: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        # logging\n",
    "        if step % 500 == 0:\n",
    "            print('Mini-batch train at step {} with loss: {}'.format(step, l))\n",
    "            print('Mini-batch train accuracy: {}, valid accuracy: {}'.format(\n",
    "                 accuracy(predictions, batch_labels),\n",
    "                 accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print('Mini-batch test accuarcy: {}'.format(accuracy(test_prediction.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Drop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "graph = tf.Graph()\n",
    "h1 = 1024\n",
    "beta = 0.01\n",
    "num_steps = 30001\n",
    "learn_rate = 0.5\n",
    "keep_prob = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initilazed\n",
      "Mini-batch train at step 0 with loss: 3604.9814453125\n",
      "Mini-batch train accuracy: 9.375, valid accuracy: 34.2\n",
      "Mini-batch train at step 1000 with loss: 0.9797767400741577\n",
      "Mini-batch train accuracy: 82.8125, valid accuracy: 83.46\n",
      "Mini-batch train at step 2000 with loss: 0.6209619045257568\n",
      "Mini-batch train accuracy: 86.71875, valid accuracy: 81.92\n",
      "Mini-batch train at step 3000 with loss: 0.899412989616394\n",
      "Mini-batch train accuracy: 80.46875, valid accuracy: 82.94\n",
      "Mini-batch train at step 4000 with loss: 0.7836065888404846\n",
      "Mini-batch train accuracy: 81.25, valid accuracy: 82.08\n",
      "Mini-batch train at step 5000 with loss: 0.9031695127487183\n",
      "Mini-batch train accuracy: 84.375, valid accuracy: 82.9\n",
      "Mini-batch train at step 6000 with loss: 0.7172757983207703\n",
      "Mini-batch train accuracy: 83.59375, valid accuracy: 83.49\n",
      "Mini-batch train at step 7000 with loss: 0.8949711918830872\n",
      "Mini-batch train accuracy: 78.125, valid accuracy: 82.85\n",
      "Mini-batch train at step 8000 with loss: 0.7487775087356567\n",
      "Mini-batch train accuracy: 85.15625, valid accuracy: 83.97\n",
      "Mini-batch train at step 9000 with loss: 0.6796194911003113\n",
      "Mini-batch train accuracy: 83.59375, valid accuracy: 83.46\n",
      "Mini-batch train at step 10000 with loss: 0.8772481083869934\n",
      "Mini-batch train accuracy: 77.34375, valid accuracy: 83.65\n",
      "Mini-batch train at step 11000 with loss: 0.7490832209587097\n",
      "Mini-batch train accuracy: 82.03125, valid accuracy: 83.92\n",
      "Mini-batch train at step 12000 with loss: 0.6822193264961243\n",
      "Mini-batch train accuracy: 88.28125, valid accuracy: 83.63\n",
      "Mini-batch train at step 13000 with loss: 0.7690601944923401\n",
      "Mini-batch train accuracy: 84.375, valid accuracy: 83.11\n",
      "Mini-batch train at step 14000 with loss: 0.7745177149772644\n",
      "Mini-batch train accuracy: 82.8125, valid accuracy: 83.51\n",
      "Mini-batch train at step 15000 with loss: 0.8144321441650391\n",
      "Mini-batch train accuracy: 82.03125, valid accuracy: 83.84\n",
      "Mini-batch train at step 16000 with loss: 0.8719975352287292\n",
      "Mini-batch train accuracy: 81.25, valid accuracy: 82.78\n",
      "Mini-batch train at step 17000 with loss: 0.9625611305236816\n",
      "Mini-batch train accuracy: 78.90625, valid accuracy: 84.0\n",
      "Mini-batch train at step 18000 with loss: 0.9886446595191956\n",
      "Mini-batch train accuracy: 75.0, valid accuracy: 83.25\n",
      "Mini-batch train at step 19000 with loss: 0.8511285781860352\n",
      "Mini-batch train accuracy: 82.03125, valid accuracy: 82.42\n",
      "Mini-batch train at step 20000 with loss: 0.6674128770828247\n",
      "Mini-batch train accuracy: 86.71875, valid accuracy: 83.35\n",
      "Mini-batch train at step 21000 with loss: 0.7875677347183228\n",
      "Mini-batch train accuracy: 85.9375, valid accuracy: 83.78\n",
      "Mini-batch train at step 22000 with loss: 0.6645761728286743\n",
      "Mini-batch train accuracy: 87.5, valid accuracy: 83.51\n",
      "Mini-batch train at step 23000 with loss: 0.8315931558609009\n",
      "Mini-batch train accuracy: 76.5625, valid accuracy: 83.01\n",
      "Mini-batch train at step 24000 with loss: 0.855012834072113\n",
      "Mini-batch train accuracy: 82.8125, valid accuracy: 83.79\n",
      "Mini-batch train at step 25000 with loss: 0.7995152473449707\n",
      "Mini-batch train accuracy: 82.03125, valid accuracy: 82.56\n",
      "Mini-batch train at step 26000 with loss: 0.6865932941436768\n",
      "Mini-batch train accuracy: 85.15625, valid accuracy: 83.67\n",
      "Mini-batch train at step 27000 with loss: 0.8401573300361633\n",
      "Mini-batch train accuracy: 82.03125, valid accuracy: 83.51\n",
      "Mini-batch train at step 28000 with loss: 0.6628847122192383\n",
      "Mini-batch train accuracy: 85.9375, valid accuracy: 83.41\n",
      "Mini-batch train at step 29000 with loss: 0.8345263004302979\n",
      "Mini-batch train accuracy: 81.25, valid accuracy: 82.53\n",
      "Mini-batch train at step 30000 with loss: 0.8584893941879272\n",
      "Mini-batch train accuracy: 81.25, valid accuracy: 83.14\n",
      "Mini-batch test accuarcy: 88.93\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    # input, only train is with SGD\n",
    "    tf_train_data = tf.placeholder(tf.float32, shape=(batch_size, image_size**2))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, class_num))\n",
    "    tf_valid_data = tf.constant(valid_data)\n",
    "    tf_test_data = tf.constant(test_data)\n",
    "    \n",
    "    # variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size**2, h1]))\n",
    "    biases1 = tf.Variable(tf.zeros(h1))  \n",
    "    weights2 = tf.Variable(tf.truncated_normal([h1, class_num]))\n",
    "    biases2 = tf.Variable(tf.zeros(class_num))  \n",
    "    \n",
    "    # train compution   \n",
    "    logits_1 = tf.matmul(tf_train_data, weights1) + biases1\n",
    "    relu_layer = tf.nn.relu(logits_1)\n",
    "    dropout_layer = tf.nn.dropout(relu_layer, keep_prob=keep_prob)\n",
    "    logits_2 = tf.matmul(dropout_layer, weights2) + biases2\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits_2))\n",
    "    regularization = beta*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(biases1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(biases2))\n",
    "    loss = tf.reduce_mean(loss + regularization)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss)\n",
    "    \n",
    "    # prediction\n",
    "    train_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    logits_1 = tf.matmul(tf_valid_data, weights1) + biases1\n",
    "    relu_layer = tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights2) + biases2\n",
    "    valid_prediction = tf.nn.softmax(logits_2)\n",
    "\n",
    "    logits_1 = tf.matmul(tf_test_data, weights1) + biases1\n",
    "    relu_layer = tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights2) + biases2\n",
    "    test_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "with tf.Session(graph=g) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initilazed')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step*batch_size)%(train_labels.shape[0]-batch_size)\n",
    "        # generate a mini-batch\n",
    "        batch_data = train_data[offset:(offset+batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size), :]\n",
    "        \n",
    "        # dict to feed mini-batch\n",
    "        feed_dict = {tf_train_data: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        # logging\n",
    "        if step % 1000 == 0:\n",
    "            print('Mini-batch train at step {} with loss: {}'.format(step, l))\n",
    "            print('Mini-batch train accuracy: {}, valid accuracy: {}'.format(\n",
    "                 accuracy(predictions, batch_labels),\n",
    "                 accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print('Mini-batch test accuarcy: {}'.format(accuracy(test_prediction.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initilazed\n",
      "Mini-batch train at step 0 with loss: 3607.59912109375\n",
      "Mini-batch train accuracy: 7.8125, valid accuracy: 37.18\n",
      "Mini-batch train at step 1000 with loss: 0.5174341201782227\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.01\n",
      "Mini-batch train at step 2000 with loss: 0.39363279938697815\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.11\n",
      "Mini-batch train at step 3000 with loss: 0.3683589994907379\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.91\n",
      "Mini-batch train at step 4000 with loss: 0.34328383207321167\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.25\n",
      "Mini-batch train at step 5000 with loss: 0.4035557508468628\n",
      "Mini-batch train accuracy: 99.21875, valid accuracy: 80.73\n",
      "Mini-batch train at step 6000 with loss: 0.35773080587387085\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.94\n",
      "Mini-batch train at step 7000 with loss: 0.36978304386138916\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.14\n",
      "Mini-batch train at step 8000 with loss: 0.36052682995796204\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.04\n",
      "Mini-batch train at step 9000 with loss: 0.33581265807151794\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.95\n",
      "Mini-batch train at step 10000 with loss: 0.3772028088569641\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.73\n",
      "Mini-batch train at step 11000 with loss: 0.36700722575187683\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.98\n",
      "Mini-batch train at step 12000 with loss: 0.3895646333694458\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.17\n",
      "Mini-batch train at step 13000 with loss: 0.36668458580970764\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.03\n",
      "Mini-batch train at step 14000 with loss: 0.3811803162097931\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.22\n",
      "Mini-batch train at step 15000 with loss: 0.3626002371311188\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.89\n",
      "Mini-batch train at step 16000 with loss: 0.3802419900894165\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.85\n",
      "Mini-batch train at step 17000 with loss: 0.37156620621681213\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.05\n",
      "Mini-batch train at step 18000 with loss: 0.3556315302848816\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.21\n",
      "Mini-batch train at step 19000 with loss: 0.5718640089035034\n",
      "Mini-batch train accuracy: 92.1875, valid accuracy: 79.51\n",
      "Mini-batch train at step 20000 with loss: 0.35604286193847656\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.11\n",
      "Mini-batch train at step 21000 with loss: 0.37907636165618896\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.01\n",
      "Mini-batch train at step 22000 with loss: 0.3585589528083801\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.79\n",
      "Mini-batch train at step 23000 with loss: 0.3478773832321167\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.26\n",
      "Mini-batch train at step 24000 with loss: 0.40934300422668457\n",
      "Mini-batch train accuracy: 99.21875, valid accuracy: 80.48\n",
      "Mini-batch train at step 25000 with loss: 0.3658196032047272\n",
      "Mini-batch train accuracy: 99.21875, valid accuracy: 80.56\n",
      "Mini-batch train at step 26000 with loss: 0.3710173964500427\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 81.05\n",
      "Mini-batch train at step 27000 with loss: 0.36559003591537476\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.79\n",
      "Mini-batch train at step 28000 with loss: 0.34449219703674316\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.9\n",
      "Mini-batch train at step 29000 with loss: 0.3706133961677551\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.6\n",
      "Mini-batch train at step 30000 with loss: 0.376255601644516\n",
      "Mini-batch train accuracy: 100.0, valid accuracy: 80.88\n",
      "Mini-batch test accuarcy: 87.58\n"
     ]
    }
   ],
   "source": [
    "train_data2 = train_data[:1000, :]\n",
    "train_labels2 = train_labels[:1000, :]\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    # input, only train is with SGD\n",
    "    tf_train_data = tf.placeholder(tf.float32, shape=(batch_size, image_size**2))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, class_num))\n",
    "    tf_valid_data = tf.constant(valid_data)\n",
    "    tf_test_data = tf.constant(test_data)\n",
    "    \n",
    "    # variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size**2, h1]))\n",
    "    biases1 = tf.Variable(tf.zeros(h1))  \n",
    "    weights2 = tf.Variable(tf.truncated_normal([h1, class_num]))\n",
    "    biases2 = tf.Variable(tf.zeros(class_num))  \n",
    "    \n",
    "    # train compution   \n",
    "    logits_1 = tf.matmul(tf_train_data, weights1) + biases1\n",
    "    relu_layer = tf.nn.relu(logits_1)\n",
    "    dropout_layer = tf.nn.dropout(relu_layer, keep_prob=keep_prob)\n",
    "    logits_2 = tf.matmul(dropout_layer, weights2) + biases2\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits_2))\n",
    "    regularization = beta*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(biases1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(biases2))\n",
    "    loss = tf.reduce_mean(loss + regularization)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss)\n",
    "    \n",
    "    # prediction\n",
    "    train_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    logits_1 = tf.matmul(tf_valid_data, weights1) + biases1\n",
    "    relu_layer = tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights2) + biases2\n",
    "    valid_prediction = tf.nn.softmax(logits_2)\n",
    "\n",
    "    logits_1 = tf.matmul(tf_test_data, weights1) + biases1\n",
    "    relu_layer = tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights2) + biases2\n",
    "    test_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "with tf.Session(graph=g) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initilazed')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step*batch_size)%(train_labels2.shape[0]-batch_size)\n",
    "        # generate a mini-batch\n",
    "        batch_data = train_data2[offset:(offset+batch_size), :]\n",
    "        batch_labels = train_labels2[offset:(offset+batch_size), :]\n",
    "        \n",
    "        # dict to feed mini-batch\n",
    "        feed_dict = {tf_train_data: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        # logging\n",
    "        if step % 1000 == 0:\n",
    "            print('Mini-batch train at step {} with loss: {}'.format(step, l))\n",
    "            print('Mini-batch train accuracy: {}, valid accuracy: {}'.format(\n",
    "                 accuracy(predictions, batch_labels),\n",
    "                 accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print('Mini-batch test accuarcy: {}'.format(accuracy(test_prediction.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Imporve with deeper NN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Try high level API: Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    '''Model function of CNN'''\n",
    "    \n",
    "    # Input layer\n",
    "    input_layer = tf.reshape(features['x'], [-1, 28, 28, 1])\n",
    "    print(input_layer.shape)\n",
    "    \n",
    "    # Convolutional layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs = input_layer, \n",
    "        filters=32,\n",
    "        kernel_size=[5, 5], \n",
    "        padding='same', \n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    # Pooling layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    # Convolutional layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1, \n",
    "        filters=64, \n",
    "        kernel_size=[5, 5], \n",
    "        padding='same', \n",
    "        activation=tf.nn.relu)\n",
    "      \n",
    "    # Pooling layer #1\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    # Dense layer #1\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7*7*64])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=0.4, \n",
    "                                training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    #Logits layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=class_num)\n",
    "    \n",
    "    predictions = {\n",
    "        'classes': tf.argmax(input=logits, axis=1),\n",
    "        'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    # Loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits))\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "        \n",
    "    eval_metric_ops = {\n",
    "        'accuracy': tf.metrics.accuracy(labels=tf.argmax(input=labels, axis=1),\n",
    "                                        predictions=predictions['classes'])\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,loss=loss,\n",
    "                                      eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_max': 5, '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_save_checkpoints_steps': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000025C81518E48>, '_evaluation_master': '', '_service': None, '_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': None, '_task_id': 0, '_save_summary_steps': 100, '_train_distribute': None, '_device_fn': None, '_task_type': 'worker', '_global_id_in_cluster': 0, '_session_config': None, '_model_dir': 'C:\\\\Users\\\\7153678\\\\Desktop\\\\AI\\\\src\\\\nlp\\\\data\\\\mnist', '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "# Create the Estimator\n",
    "mnist_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": train_data},\n",
    "    y=train_labels,\n",
    "    batch_size=100,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "(100, 28, 28, 1)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\data\\mnist\\model.ckpt-1001\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1001 into C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\data\\mnist\\model.ckpt.\n",
      "INFO:tensorflow:probabilities = [[0.15792854 0.08667378 0.09345014 0.0884434  0.08824697 0.09821492\n",
      "  0.08851908 0.09016933 0.09645264 0.11190128]\n",
      " [0.07136316 0.08533433 0.15975645 0.12650672 0.0976068  0.13163954\n",
      "  0.10275698 0.09097126 0.06976172 0.06430311]\n",
      " [0.08096241 0.0797606  0.12615493 0.07251994 0.12678356 0.11385694\n",
      "  0.11093532 0.07703714 0.1448418  0.0671474 ]\n",
      " [0.11842261 0.1321772  0.09319384 0.11175149 0.09167321 0.10226443\n",
      "  0.07473855 0.09487867 0.06182375 0.11907618]\n",
      " [0.07481819 0.16958211 0.09796752 0.10176328 0.10204814 0.08235142\n",
      "  0.11007311 0.13911347 0.04879987 0.07348285]\n",
      " [0.05644123 0.12907848 0.10481712 0.12865216 0.10611815 0.06308944\n",
      "  0.15640655 0.1146374  0.07148837 0.06927122]\n",
      " [0.08405957 0.15221079 0.09362114 0.09207898 0.10276175 0.07552221\n",
      "  0.09875113 0.1381257  0.07419422 0.08867445]\n",
      " [0.10785473 0.08299781 0.1030322  0.10271436 0.11450472 0.11958507\n",
      "  0.08072834 0.0989265  0.09021017 0.09944614]\n",
      " [0.07717902 0.09086799 0.14566039 0.12820742 0.08733457 0.07033643\n",
      "  0.10372782 0.11730701 0.08078165 0.09859779]\n",
      " [0.05454938 0.08287899 0.13651477 0.0841637  0.17998296 0.12320909\n",
      "  0.10435128 0.1060449  0.06822105 0.06008375]\n",
      " [0.09945054 0.11982706 0.0911179  0.1313839  0.0727746  0.07948487\n",
      "  0.08744717 0.09812445 0.13730049 0.08308899]\n",
      " [0.10133459 0.09995974 0.13336985 0.1120763  0.11247857 0.0944651\n",
      "  0.10855853 0.10208736 0.06448417 0.07118578]\n",
      " [0.10289513 0.1114089  0.09148208 0.11079533 0.08095771 0.0962602\n",
      "  0.10713296 0.09015406 0.09404897 0.11486468]\n",
      " [0.0978004  0.12329511 0.09058036 0.12228446 0.1073124  0.08095892\n",
      "  0.09235274 0.10547028 0.08783853 0.09210674]\n",
      " [0.11268435 0.07064715 0.12490848 0.11876997 0.09318012 0.12123268\n",
      "  0.11237903 0.06268533 0.09668092 0.08683197]\n",
      " [0.08227841 0.10639887 0.08863087 0.0888999  0.1088489  0.16155866\n",
      "  0.0892624  0.08597711 0.09890173 0.08924328]\n",
      " [0.10510215 0.10045034 0.10875779 0.10260169 0.07467604 0.14928637\n",
      "  0.09683007 0.0875321  0.07717445 0.09758883]\n",
      " [0.07791233 0.11996796 0.10945275 0.11115968 0.10972088 0.08614934\n",
      "  0.16738701 0.11197665 0.05250522 0.05376814]\n",
      " [0.10671413 0.11362053 0.06933834 0.09685721 0.11166639 0.11244115\n",
      "  0.07976411 0.08223215 0.1010948  0.1262712 ]\n",
      " [0.08966503 0.11081714 0.10798306 0.11199436 0.09188489 0.13086762\n",
      "  0.10524493 0.09425725 0.07186034 0.08542541]\n",
      " [0.08851878 0.10748901 0.06831212 0.09872216 0.10745622 0.06564586\n",
      "  0.10838109 0.0929624  0.0783803  0.18413216]\n",
      " [0.17637652 0.07016441 0.12846437 0.0788554  0.08154345 0.10848817\n",
      "  0.06458156 0.05864159 0.12920442 0.10368012]\n",
      " [0.10017215 0.10272732 0.08817263 0.12173169 0.08513577 0.10312987\n",
      "  0.11617179 0.09711795 0.07917161 0.10646918]\n",
      " [0.06071427 0.06643334 0.16976097 0.11166404 0.0777138  0.09435327\n",
      "  0.1410804  0.11624014 0.08358441 0.07845535]\n",
      " [0.08086263 0.10946403 0.09821417 0.09508471 0.07618069 0.10405812\n",
      "  0.09975224 0.1609444  0.0676666  0.10777244]\n",
      " [0.14741617 0.09164795 0.0800894  0.09183288 0.06770105 0.07327109\n",
      "  0.07710857 0.07708908 0.14445396 0.14938994]\n",
      " [0.15911089 0.0800504  0.06929215 0.08099776 0.08514547 0.08147953\n",
      "  0.08338743 0.0700364  0.14520682 0.14529315]\n",
      " [0.14468426 0.1056064  0.05921466 0.0892544  0.10336327 0.08665159\n",
      "  0.07617602 0.05034639 0.16514464 0.11955837]\n",
      " [0.12903279 0.10188325 0.09742921 0.11380784 0.09016671 0.0941648\n",
      "  0.10345916 0.09200907 0.07749335 0.10055377]\n",
      " [0.07140959 0.09827198 0.10771544 0.14464308 0.1378992  0.06402116\n",
      "  0.1512518  0.09658109 0.04983652 0.07837009]\n",
      " [0.14021246 0.09562734 0.08514544 0.12260563 0.069799   0.0840977\n",
      "  0.09209793 0.1003238  0.09243659 0.11765411]\n",
      " [0.13033871 0.08069342 0.07417876 0.0896963  0.10671037 0.09073244\n",
      "  0.1356165  0.09612233 0.08708455 0.10882661]\n",
      " [0.07355105 0.18078063 0.0871271  0.09420906 0.11168195 0.06120323\n",
      "  0.11589572 0.11034222 0.09882149 0.06638756]\n",
      " [0.08275523 0.09097595 0.10721619 0.15286031 0.10797632 0.05738594\n",
      "  0.1351615  0.11497138 0.07346216 0.07723501]\n",
      " [0.06437042 0.11305018 0.09183684 0.09834013 0.09878255 0.07253011\n",
      "  0.11837445 0.1868853  0.07004703 0.08578298]\n",
      " [0.14326859 0.08523115 0.08954713 0.08317924 0.08653435 0.1092229\n",
      "  0.09126461 0.08336501 0.0766472  0.15173979]\n",
      " [0.10861149 0.07303791 0.07061161 0.08807606 0.09804029 0.12450915\n",
      "  0.12936556 0.08506513 0.07976657 0.14291626]\n",
      " [0.1735937  0.06861883 0.08535899 0.12830101 0.10503814 0.09671345\n",
      "  0.09302999 0.0850791  0.08223369 0.08203314]\n",
      " [0.10213406 0.10297194 0.10559245 0.08545797 0.10371956 0.07623903\n",
      "  0.09009872 0.09994511 0.16421019 0.06963094]\n",
      " [0.08416024 0.11251858 0.10763187 0.11010706 0.09903546 0.07933369\n",
      "  0.10727815 0.11362685 0.10768805 0.07862009]\n",
      " [0.09110703 0.08727894 0.11950022 0.08875981 0.09857404 0.09106718\n",
      "  0.10416226 0.08126193 0.1059337  0.1323549 ]\n",
      " [0.08210173 0.08814351 0.12407206 0.09732122 0.1213114  0.09170195\n",
      "  0.091966   0.09319264 0.1349529  0.07523664]\n",
      " [0.11583487 0.09045709 0.08815976 0.09166113 0.10048859 0.10353518\n",
      "  0.1285978  0.096599   0.08506622 0.0996004 ]\n",
      " [0.12853019 0.10567557 0.09911826 0.09924828 0.08030409 0.08866997\n",
      "  0.09840558 0.07234644 0.10268594 0.12501572]\n",
      " [0.10892753 0.08824118 0.0970118  0.08880375 0.10647979 0.095783\n",
      "  0.09516113 0.0818427  0.093807   0.1439421 ]\n",
      " [0.11807302 0.07974669 0.1164808  0.09479963 0.07913203 0.07321968\n",
      "  0.08281841 0.06226557 0.12913656 0.16432756]\n",
      " [0.08915696 0.10137568 0.1289189  0.08240157 0.11124416 0.18028344\n",
      "  0.09011544 0.07389013 0.07486127 0.0677524 ]\n",
      " [0.08907268 0.07278945 0.10394129 0.12476984 0.08023414 0.06935858\n",
      "  0.12583055 0.15257022 0.08425017 0.09718306]\n",
      " [0.09335435 0.08144718 0.13910076 0.08648591 0.12783003 0.11604871\n",
      "  0.09287365 0.08877368 0.07266681 0.10141886]\n",
      " [0.07169364 0.12465241 0.1174736  0.09100196 0.17655973 0.09958279\n",
      "  0.10028993 0.07412016 0.06860518 0.07602059]\n",
      " [0.11874949 0.09741089 0.08878445 0.08353622 0.08102707 0.0878051\n",
      "  0.05189864 0.07304461 0.15147264 0.16627093]\n",
      " [0.10547859 0.12899758 0.06827669 0.10262375 0.07392243 0.05990093\n",
      "  0.09935439 0.09457014 0.07256588 0.19430958]\n",
      " [0.08615569 0.11214868 0.11130397 0.13217273 0.08163179 0.0686982\n",
      "  0.1125652  0.14230555 0.07826799 0.07475024]\n",
      " [0.16243015 0.10448111 0.09802636 0.07192987 0.09128313 0.08774669\n",
      "  0.05908277 0.06414153 0.13404886 0.12682946]\n",
      " [0.07589979 0.09733488 0.12574421 0.08669624 0.11995018 0.15979095\n",
      "  0.08239283 0.09072281 0.10345556 0.05801253]\n",
      " [0.06398068 0.08866879 0.22436152 0.12087871 0.10420701 0.08526083\n",
      "  0.08635481 0.07893263 0.06097471 0.08638047]\n",
      " [0.09490637 0.11177478 0.12975325 0.12102799 0.09707807 0.0851237\n",
      "  0.10387284 0.09116089 0.07564744 0.08965459]\n",
      " [0.06669827 0.08364066 0.12995945 0.1291496  0.15168335 0.0853077\n",
      "  0.12754476 0.08411773 0.05863256 0.08326585]\n",
      " [0.11767163 0.11176939 0.08643112 0.06912445 0.08522794 0.10175461\n",
      "  0.10306363 0.08044161 0.09420858 0.15030703]\n",
      " [0.07919303 0.13980338 0.11819008 0.10956282 0.09523891 0.07855513\n",
      "  0.14681353 0.11573979 0.06815896 0.04874445]\n",
      " [0.1080255  0.0883582  0.07782302 0.11745778 0.10438111 0.09820217\n",
      "  0.10591324 0.08679667 0.07258918 0.14045312]\n",
      " [0.08206744 0.09995717 0.14496727 0.10059612 0.12466711 0.09305893\n",
      "  0.11744979 0.09547897 0.07308807 0.06866912]\n",
      " [0.09204532 0.11715983 0.06028946 0.11815649 0.11706507 0.09371327\n",
      "  0.1381671  0.11595321 0.06394764 0.08350268]\n",
      " [0.09544607 0.11744566 0.07763227 0.11569846 0.11121815 0.15075383\n",
      "  0.07420238 0.08650024 0.0874811  0.08362191]\n",
      " [0.08135422 0.12843002 0.07956351 0.1188847  0.11725884 0.08755366\n",
      "  0.10275215 0.10900196 0.08188809 0.09331294]\n",
      " [0.16243726 0.11666625 0.07089003 0.09741071 0.07598693 0.06806514\n",
      "  0.08171222 0.11495474 0.10965607 0.10222073]\n",
      " [0.102892   0.11602242 0.11092122 0.07946752 0.1265585  0.07824933\n",
      "  0.09281665 0.09752142 0.11934693 0.07620399]\n",
      " [0.09467324 0.08317637 0.12449525 0.08267074 0.12088016 0.1812869\n",
      "  0.06366617 0.08708791 0.08388478 0.07817851]\n",
      " [0.13117526 0.10175353 0.06758529 0.07215482 0.0811798  0.09606246\n",
      "  0.06633019 0.05769696 0.2062918  0.11976992]\n",
      " [0.07543324 0.09451936 0.10452206 0.1338603  0.10282799 0.11103157\n",
      "  0.09755647 0.13904826 0.07116974 0.07003096]\n",
      " [0.1412119  0.09953006 0.11498384 0.05301166 0.06970774 0.08379657\n",
      "  0.08796535 0.05435523 0.18186176 0.11357584]\n",
      " [0.09242493 0.10613272 0.11453741 0.07734695 0.10801343 0.14096016\n",
      "  0.07713925 0.09446923 0.11629573 0.07268015]\n",
      " [0.05789852 0.11092575 0.13992564 0.137877   0.08812079 0.10244439\n",
      "  0.11252379 0.13225663 0.0551555  0.062872  ]\n",
      " [0.12271281 0.11877341 0.0656518  0.06390989 0.09085207 0.06031009\n",
      "  0.09508317 0.12179558 0.11980377 0.14110744]\n",
      " [0.09899574 0.09788626 0.09818376 0.0865144  0.10780212 0.07884607\n",
      "  0.10684204 0.12920797 0.09538366 0.10033792]\n",
      " [0.14700681 0.07979808 0.0712276  0.08547095 0.08570419 0.0607461\n",
      "  0.09587062 0.10234591 0.11141142 0.16041836]\n",
      " [0.13948765 0.08345082 0.09023894 0.09621286 0.07635251 0.05547978\n",
      "  0.09065149 0.10259508 0.1075191  0.1580118 ]\n",
      " [0.128889   0.08368579 0.09337225 0.11875711 0.075057   0.08420604\n",
      "  0.13036677 0.10482221 0.05036267 0.1304811 ]\n",
      " [0.08663081 0.12598349 0.11007486 0.08693136 0.13306744 0.08475238\n",
      "  0.0901596  0.11845186 0.08257485 0.08137346]\n",
      " [0.07495541 0.12177191 0.11196247 0.12072812 0.10079134 0.08436867\n",
      "  0.12234873 0.10872857 0.07097133 0.08337346]\n",
      " [0.0832276  0.11914204 0.13369018 0.06907292 0.16505729 0.13412265\n",
      "  0.08995561 0.07736003 0.07460807 0.0537636 ]\n",
      " [0.05527631 0.06743378 0.2251778  0.0960771  0.08639697 0.09764928\n",
      "  0.11799902 0.06909623 0.07563438 0.10925907]\n",
      " [0.1474066  0.05916636 0.08180799 0.07680265 0.10148499 0.09455126\n",
      "  0.08113094 0.05101347 0.15906997 0.14756571]\n",
      " [0.07922315 0.11618379 0.10038511 0.13526723 0.12894472 0.08452274\n",
      "  0.10390702 0.06229942 0.10795887 0.08130797]\n",
      " [0.19977412 0.10488945 0.06927004 0.09595142 0.07094709 0.05427373\n",
      "  0.07199015 0.12213276 0.10163772 0.10913339]\n",
      " [0.09326359 0.06859711 0.13142434 0.06016653 0.10516808 0.0778531\n",
      "  0.07966737 0.07370409 0.21391569 0.09624016]\n",
      " [0.10306285 0.11648504 0.14381288 0.09106271 0.12327425 0.11707088\n",
      "  0.11433518 0.07688713 0.05770121 0.05630781]\n",
      " [0.11775531 0.09684204 0.08335197 0.08370172 0.08313849 0.07549693\n",
      "  0.0763453  0.12582394 0.11880334 0.13874097]\n",
      " [0.14169653 0.10200919 0.09699427 0.10775971 0.11992849 0.05708469\n",
      "  0.13413866 0.07124125 0.08729526 0.08185197]\n",
      " [0.1377363  0.09326832 0.08075694 0.09621142 0.10186218 0.07004157\n",
      "  0.06699946 0.07085469 0.11596192 0.16630712]\n",
      " [0.17592545 0.11636386 0.07490589 0.08966988 0.10689595 0.08621389\n",
      "  0.05962325 0.07528094 0.10348471 0.11163621]\n",
      " [0.12927912 0.12274463 0.11254905 0.10593417 0.09515782 0.07915413\n",
      "  0.08792192 0.11737482 0.09262156 0.05726277]\n",
      " [0.08769611 0.16674513 0.07736947 0.12389039 0.07183585 0.08517651\n",
      "  0.0869094  0.16124657 0.08120669 0.05792376]\n",
      " [0.09508426 0.12365399 0.09769135 0.11767272 0.10657372 0.07092469\n",
      "  0.118345   0.10720862 0.08908385 0.07376184]\n",
      " [0.06322243 0.10673601 0.11661813 0.14866185 0.10428227 0.11467299\n",
      "  0.11106195 0.10200402 0.05658695 0.07615354]\n",
      " [0.06924225 0.08693479 0.19524333 0.126618   0.08213105 0.07928026\n",
      "  0.12069925 0.06496689 0.08947403 0.08541024]\n",
      " [0.08359247 0.07396844 0.15252292 0.08045754 0.12496988 0.10349568\n",
      "  0.1529213  0.08877246 0.07201851 0.06728075]\n",
      " [0.1616522  0.11003166 0.06116067 0.1035051  0.08262792 0.07658955\n",
      "  0.06347977 0.07780514 0.13184421 0.13130377]\n",
      " [0.10707387 0.10113774 0.08387939 0.10439079 0.10173434 0.08576562\n",
      "  0.12517506 0.1132952  0.10946584 0.06808209]\n",
      " [0.08014213 0.10826793 0.11573222 0.13377501 0.12708803 0.08194701\n",
      "  0.11222637 0.08103589 0.09514912 0.06463633]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 2.0368164, step = 1002\n",
      "INFO:tensorflow:Saving checkpoints for 1002 into C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\data\\mnist\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.0368164.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x25c815186a0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train one step and display the probabilties\n",
    "mnist_classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=1,\n",
    "    hooks=[logging_hook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "(100, 28, 28, 1)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\data\\mnist\\model.ckpt-1002\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1002 into C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\data\\mnist\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.0138888, step = 1003\n",
      "INFO:tensorflow:global_step/sec: 2.03799\n",
      "INFO:tensorflow:loss = 0.7248892, step = 1103 (49.069 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12888\n",
      "INFO:tensorflow:loss = 0.68806016, step = 1203 (46.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.02098\n",
      "INFO:tensorflow:loss = 0.64603543, step = 1303 (49.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.29268\n",
      "INFO:tensorflow:loss = 0.465855, step = 1403 (43.610 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.28331\n",
      "INFO:tensorflow:loss = 0.46244156, step = 1503 (43.797 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08221\n",
      "INFO:tensorflow:loss = 0.7857921, step = 1603 (48.030 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.21887\n",
      "INFO:tensorflow:loss = 0.5571206, step = 1703 (45.063 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.32569\n",
      "INFO:tensorflow:loss = 0.3476203, step = 1803 (42.999 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.18488\n",
      "INFO:tensorflow:loss = 0.3648517, step = 1903 (45.776 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2002 into C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\data\\mnist\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.57788134.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x25c815186a0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_classifier.train(input_fn=train_input_fn, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "(?, 28, 28, 1)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-09-06-14:59:57\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\data\\mnist\\model.ckpt-2002\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-09-06-15:00:11\n",
      "INFO:tensorflow:Saving dict for global step 2002: accuracy = 0.9343, global_step = 2002, loss = 0.22989973\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2002: C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\data\\mnist\\model.ckpt-2002\n",
      "{'global_step': 2002, 'accuracy': 0.9343, 'loss': 0.22989973}\n"
     ]
    }
   ],
   "source": [
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": test_data},\n",
    "    y=test_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "test_results = mnist_classifier.evaluate(input_fn=test_input_fn)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
