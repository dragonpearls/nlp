{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import orm\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Column, Integer, String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyltp\n",
    "from pyltp import SentenceSplitter\n",
    "from pyltp import Segmentor\n",
    "from pyltp import Postagger\n",
    "from pyltp import NamedEntityRecognizer\n",
    "from pyltp import Parser\n",
    "from pyltp import SementicRole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from DB server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据库地址（Host）rm-8vbwj6507z6465505ro.mysql.zhangbei.rds.aliyuncs.com\n",
    "- 用户名（User）：root\n",
    "- 用户密码（Password）：AI@2019@ai\n",
    "- 数据库名（Database）：stu_db\n",
    "- 表名：news_chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = {\n",
    "    'database': 'stu_db',\n",
    "    'drivername': 'mysql+mysqldb',\n",
    "    'username': 'root',\n",
    "    'password': 'AI@2019@ai',\n",
    "    'host': 'rm-8vbwj6507z6465505ro.mysql.zhangbei.rds.aliyuncs.com',\n",
    "    'query': {'charset': 'utf8'},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mysql+mysqldb://root:***@rm-8vbwj6507z6465505ro.mysql.zhangbei.rds.aliyuncs.com/stu_db?charset=utf8"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = URL(**db_url)\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(url, echo=True, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 19:40:13,431 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'sql_mode'\n",
      "2019-08-16 19:40:13,434 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-16 19:40:13,542 INFO sqlalchemy.engine.base.Engine SELECT DATABASE()\n",
      "2019-08-16 19:40:13,545 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-16 19:40:13,660 INFO sqlalchemy.engine.base.Engine show collation where `Charset` = 'utf8' and `Collation` = 'utf8_bin'\n",
      "2019-08-16 19:40:13,662 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-16 19:40:13,729 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1\n",
      "2019-08-16 19:40:13,732 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-16 19:40:13,790 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1\n",
      "2019-08-16 19:40:13,794 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-16 19:40:13,849 INFO sqlalchemy.engine.base.Engine SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8) COLLATE utf8_bin AS anon_1\n",
      "2019-08-16 19:40:13,852 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    }
   ],
   "source": [
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 19:40:27,343 INFO sqlalchemy.engine.base.Engine show tables\n",
      "2019-08-16 19:40:27,345 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception during reset or similar\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sqlalchemy\\pool.py\", line 709, in _finalize_fairy\n",
      "    fairy._reset(pool)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sqlalchemy\\pool.py\", line 880, in _reset\n",
      "    pool._dialect.do_rollback(self)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sqlalchemy\\dialects\\mysql\\base.py\", line 1796, in do_rollback\n",
      "    dbapi_connection.rollback()\n",
      "_mysql_exceptions.OperationalError: (2006, 'MySQL server has gone away')\n"
     ]
    }
   ],
   "source": [
    "tables = connection.execute('show tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 19:41:18,564 INFO sqlalchemy.engine.base.Engine show full columns from news_chinese\n",
      "2019-08-16 19:41:18,566 INFO sqlalchemy.engine.base.Engine ()\n",
      "('id', 'int(11)', None, 'NO', 'PRI', None, '', 'select,insert,update,references', '')\n",
      "('author', 'varchar(32)', 'utf8_general_ci', 'YES', '', None, '', 'select,insert,update,references', '')\n",
      "('source', 'varchar(32)', 'utf8_general_ci', 'YES', '', None, '', 'select,insert,update,references', '')\n",
      "('content', 'varchar(1000)', 'utf8_general_ci', 'YES', '', None, '', 'select,insert,update,references', '')\n",
      "('feature', 'varchar(256)', 'utf8_general_ci', 'YES', '', None, '', 'select,insert,update,references', '')\n",
      "('title', 'varchar(32)', 'utf8_general_ci', 'YES', '', None, '', 'select,insert,update,references', '')\n",
      "('url', 'varchar(32)', 'utf8_general_ci', 'YES', '', None, '', 'select,insert,update,references', '')\n"
     ]
    }
   ],
   "source": [
    "fileds = connection.execute('show full columns from news_chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 19:43:15,663 INFO sqlalchemy.engine.base.Engine select count(*) from news_chinese\n",
      "2019-08-16 19:43:15,665 INFO sqlalchemy.engine.base.Engine ()\n",
      "(89611,)\n"
     ]
    }
   ],
   "source": [
    "count = connection.execute('select count(*) from news_chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 19:44:36,633 INFO sqlalchemy.engine.base.Engine select * from news_chinese limit 5\n",
      "2019-08-16 19:44:36,635 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-16 19:44:36,771 INFO sqlalchemy.engine.base.Engine select * from news_chinese_backup limit 5\n",
      "2019-08-16 19:44:36,773 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    }
   ],
   "source": [
    "query = 'select * from news_chinese limit 5'\n",
    "df = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['id', 'author', 'source', 'content', 'feature', 'title', 'url'], dtype='object'),\n",
       " RangeIndex(start=0, stop=5, step=1))"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns, df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"type\":\"体育\",\"site\":\"新华社\",\"url\":\"http://home.xinhua-news.com/gdsdetailxhsnew/2203534-?pageflag=init&imageOrVedioTypeGdsId=\",\"site_sign\":\"xinhua\",\"keywords\":\"乒乓球\",\"summary\":\"\\u3000\\u3000新华社德国杜塞尔多夫６月６日电 题：乒乓女球迷\\u3000 \\u3000\\u3000新华社记者王子江、张寒 \\u3000\\u3000熊老师离开上海前，特意花一千多元买了一只张继科代言的球拍，准备在世界锦标赛期间他\"}'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_id1 = df.loc[0, 'feature']+ r'\"}'\n",
    "df_id1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keywords': '乒乓球',\n",
       " 'site': '新华社',\n",
       " 'site_sign': 'xinhua',\n",
       " 'summary': '\\u3000\\u3000新华社德国杜塞尔多夫６月６日电 题：乒乓女球迷\\u3000 \\u3000\\u3000新华社记者王子江、张寒 \\u3000\\u3000熊老师离开上海前，特意花一千多元买了一只张继科代言的球拍，准备在世界锦标赛期间他',\n",
       " 'type': '体育',\n",
       " 'url': 'http://home.xinhua-news.com/gdsdetailxhsnew/2203534-?pageflag=init&imageOrVedioTypeGdsId='}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_id1 = json.loads(df_id1)\n",
    "df_id1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 21:49:26,250 INFO sqlalchemy.engine.base.Engine select * from news_chinese\n",
      "2019-08-16 21:49:26,259 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    }
   ],
   "source": [
    "query = 'select * from news_chinese'\n",
    "df = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('news_chinese.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class News(Base):\n",
    "    __tablename__ = 'news_chinese'\n",
    "     \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    author = Column(String)\n",
    "    source = Column(String)    \n",
    "    content = Column(String)    \n",
    "    feature = Column(String)   \n",
    "    title = Column(String)        \n",
    "    url = Column(String)     \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"<news_chinese(id='%s', author='%s', title='%s')>\" % (self.id, self.author, self.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 20:08:09,589 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)\n",
      "2019-08-16 20:08:09,592 INFO sqlalchemy.engine.base.Engine SELECT news_chinese.id AS news_chinese_id, news_chinese.author AS news_chinese_author, news_chinese.source AS news_chinese_source, news_chinese.content AS news_chinese_content, news_chinese.feature AS news_chinese_feature, news_chinese.title AS news_chinese_title, news_chinese.url AS news_chinese_url \n",
      "FROM news_chinese\n",
      "2019-08-16 20:08:09,594 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    }
   ],
   "source": [
    "news_all = session.query(News).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\n新华社北京6月4日电（记者徐力宇）叙利亚库尔德人武装“人民保护部队”一名发言人3日说，夺取极端组织“伊斯兰国”位于叙北部的大本营拉卡的总攻将在“几天后”开始。\\\\n【强弩之末】\\\\n\\\\n2017年5月18日，在叙利亚拉卡附近的塔卜卡镇，两名“叙利亚民主军”士兵走在街道上。（新华/法新）\\\\n“人民保护部队”是叙利亚反对派武装“叙利亚民主军”的主力，自去年11月以来，“叙利亚民主军”已经从多方包围了拉卡。\\\\n在伊拉克，摩苏尔战役已经进入最后关头，“伊斯兰国”控制的地盘所剩无几，在叙利亚也是节节败退，拉卡之役将进一步削弱“伊斯兰国”力量。\\\\n“武装力量已经推进到拉卡市郊区，总攻将在几天后开始。”“人民保护部队”发言人努里·马哈茂德在电话采访中告诉路透社记者。\\\\n拉卡位于幼发拉底河沿岸，距离土耳其边境约90公里。此前有媒体报道，“叙利亚民主军”的发言人也表示，拉卡之役下一阶段战斗将在“几天后”开始。\\\\n由美国主导的打击“伊斯兰国”联盟发言人瑞安·狄龙拒绝置评收复拉卡行动下一阶段时间表。他说，“叙利亚民主军”每天都在向前推进，在北面和东面离拉卡市区只有3公里，在西面推进到离市区不到10公里处。\\\\n【武器到手】\\\\n\\\\n2017年5月12日，在叙利亚北部塔卜卡，“叙利亚民主军”士兵走过街头。（新华/路透）\\\\n\\\\n“叙利亚民主军”由叙利亚阿拉伯人和库尔德人组成，美国把其中的库尔德人武装“人民保护部队”视为战斗力最强的一支，并提供空中掩护，派遣军事顾问，帮助他们对抗“伊斯兰国”。\\\\n美国国防部上月宣布，总统唐纳德·特朗普已经批准向“人民保护部队”提供火力更强的武器，以确保夺下拉卡。美国军方5月30日透露，已经开始向这支武装分发武器。\\\\n土耳其强烈反对“人民保护部队”获得美援，原因是土耳其把这支库尔德人武装视为库尔德工人党的分支。库尔德工人党成立于1979年，多年来试图以武力在土耳其与伊拉克、伊朗、叙利亚交界处的库尔德人聚居区建立独立国家，被土耳其、美国和欧盟认定为恐怖组织。土耳其已经派兵进入叙北部，试图阻止“人民保护部队”扩大势力范围。\\\\n打击“伊斯兰国”联盟认为，目前还有3000到4000名“伊斯兰国”武装人员盘踞在拉卡市内，并筑起防御工事。\\\\n“战斗不会轻松，”马哈茂德说，“‘伊斯兰国’确实有地道、地雷、汽车炸弹、自杀式袭击者，同时他们还把平民当作人盾。”\\\\n'"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_all[3458].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 20:07:51,802 INFO sqlalchemy.engine.base.Engine ROLLBACK\n"
     ]
    }
   ],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data directly from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('C:/Users/7153678/Desktop/AI/src/nlp/data/news_chinese2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_nona = news.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_cnt = news_nona.iloc[:, 1:].applymap(len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_cnt.insert(0, 'id', news_nona.values[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                     87307\n",
       "author                                              廖越 PF062\n",
       "source                                                  凤凰财经\n",
       "content                                                ?\\r\\n\n",
       "feature    {\"type\":\"财经\",\"site\":\"凤凰\",\"commentNum\":\"0\",\"joi...\n",
       "title                                       一张图看懂一带一路的钱都从哪来？\n",
       "url        http://finance.ifeng.com/a/20170515/15374639_0...\n",
       "Name: 2310, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_nona[~news_nona['content'].str.strip(r'\\\\n').str.contains(r'\\w', regex=True)].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sorted = news_nona.assign(f = news_nona['content'].str.len()).sort_values('f', ascending = True).drop('f', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_big_content = news_sorted[news_sorted['content'].str.len()>=100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_web_chars(ss, chars=r'\\\\n|&nbsp|\\xa0|\\\\xa0|\\u3000|\\\\u3000|\\\\u0020|\\u0020'):\n",
    "    if isinstance(ss, str):\n",
    "        return re.sub(chars, '', ss) \n",
    "    else:\n",
    "        return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_final = news_big_content.applymap(clear_web_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新华社照片，杭州，2017年4月3日春拂西湖人如潮4月3日，杭州西湖风景区的白堤上游人如织。\n",
      "当日是清明节假期第二天，晴朗的杭州西湖风景区绿意盎然，春风拂面，游人如织。\n",
      "新华社记者黄宗治摄\n"
     ]
    }
   ],
   "source": [
    "sents = SentenceSplitter.split(news_final.iloc[15, 3])\n",
    "print('\\n'.join(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'当日是清明节假期第二天晴朗的杭州西湖风景区绿意盎然春风拂面游人如织'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = ''.join(re.findall(r'\\w+', sents[1]))\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTP_DATA_DIR = 'C:/Users/7153678/Desktop/AI/src/nlp/model/ltp_data'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`\n",
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`pos.model`\n",
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "srl_model_path = os.path.join(LTP_DATA_DIR, 'srl')  # 语义角色标注模型目录路径，模型目录为`srl`。注意该模型路径是一个目录，而不是一个文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当日 是 清明节 假期 第二 天 晴朗 的 杭州 西湖 风景区 绿意 盎然 春风 拂 面 游人 如 织\n"
     ]
    }
   ],
   "source": [
    "segmentor = Segmentor()\n",
    "segmentor.load(cws_model_path)\n",
    "words = segmentor.segment(sentence)\n",
    "print(' '.join(words))\n",
    "segmentor.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt v nt nt m q a u ns ns n n z n v n n v v\n"
     ]
    }
   ],
   "source": [
    "postagger = Postagger()\n",
    "postagger.load(pos_model_path)\n",
    "postags = postagger.postag(words)\n",
    "print(' '.join(postags))\n",
    "postagger.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O B-Ns E-Ns O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "nerecognizer = NamedEntityRecognizer()\n",
    "nerecognizer.load(ner_model_path)\n",
    "nerags = nerecognizer.recognize(words, postags)\n",
    "print(' '.join(nerags))\n",
    "nerecognizer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:SBV 当日<--是  0:HED 是<--织  4:ATT 清明节<--假期  7:ADV 假期<--晴朗  6:ATT 第二<--天  7:ADV 天<--晴朗  2:VOB 晴朗<--是  7:RAD 的<--晴朗  11:ATT 杭州<--风景区  11:ATT 西湖<--风景区  12:ATT 风景区<--绿意  13:SBV 绿意<--盎然  7:COO 盎然<--晴朗  15:SBV 春风<--拂  13:COO 拂<--盎然  15:VOB 面<--拂  19:SBV 游人<--织  19:ADV 如<--织  15:COO 织<--拂 \n"
     ]
    }
   ],
   "source": [
    "parser = Parser()\n",
    "parser.load(par_model_path)\n",
    "arcs = parser.parse(words, postags)\n",
    "print(' '.join('%d:%s %s<--%s ' % (arc.head, arc.relation, words[i], words[arc.head-1]) for i, arc in enumerate(arcs)))\n",
    "parser.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar words by W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "W2V_DATA_DIR = r'C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\model\\word2vec'  # word2vec模型目录的路径, trained by wiki\n",
    "w2v_model_path = os.path.join(W2V_DATA_DIR, 'word2vec_wiki.model')\n",
    "model = Word2Vec.load(w2v_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeate most_similar has better performance\n",
    "    # if s in seen:\n",
    "    #     seen[s] = seen[s]+1\n",
    "    # else:\n",
    "    #     candidates.append(s)\n",
    "# most_similar length 20 is better than 1o\n",
    "\n",
    "# To do...\n",
    "# optimal: 1. score function could be revised, use function of tree level and distance to root to replace seen[oldest] += ?\n",
    "# optimal: 2. using dymanic programming to reduce computing time  \n",
    "\n",
    "def get_similar_words(word, num, model):\n",
    "    \n",
    "    seen = defaultdict(int) \n",
    "    candidates = [word]   \n",
    "    \n",
    "    while len(seen) < num and candidates:\n",
    "        \n",
    "        if len(seen) % 50 == 0: \n",
    "            print('seen length : {}'.format(len(seen)))\n",
    "            \n",
    "        oldest = candidates.pop(0)\n",
    "        similars = model.most_similar(oldest, topn = 20)   \n",
    "        \n",
    "        candidates += [s for s, p in similars]       \n",
    "        seen[oldest] += 1\n",
    "        \n",
    "#         for s, p in similars:\n",
    "#             if s in seen:\n",
    "#                 seen[s] = seen[s]+1\n",
    "#             else:\n",
    "#                 candidates.append(s)\n",
    "#         seen[oldest] = 1\n",
    " \n",
    "    #similar_words = pd.DataFrame(list(seen.items()), columns=['word', 'count']).sort_values('count', ascending=False)\n",
    "    similar_words = sorted(seen.items(), key=lambda x: x[1], reverse=True)\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen length : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen length : 50\n",
      "seen length : 100\n",
      "seen length : 150\n",
      "seen length : 200\n",
      "seen length : 250\n",
      "seen length : 300\n",
      "seen length : 350\n",
      "seen length : 400\n",
      "seen length : 400\n",
      "seen length : 450\n"
     ]
    }
   ],
   "source": [
    "similar_words = get_similar_words('说',500,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('说', 15),\n",
       " ('所言', 12),\n",
       " ('说道', 11),\n",
       " ('问道', 10),\n",
       " ('答道', 10),\n",
       " ('之言', 10),\n",
       " ('地问', 9),\n",
       " ('还说', 9),\n",
       " ('转述', 9),\n",
       " ('则说', 8),\n",
       " ('讲出', 8),\n",
       " ('指出', 7),\n",
       " ('所说', 7),\n",
       " ('反问', 7),\n",
       " ('认为', 7),\n",
       " ('说过', 7),\n",
       " ('云云', 6),\n",
       " ('告诉', 6),\n",
       " ('还告诉', 6),\n",
       " ('一言', 6),\n",
       " ('并说', 6),\n",
       " ('其人', 5),\n",
       " ('不言', 5),\n",
       " ('却说', 5),\n",
       " ('此说', 5),\n",
       " ('相信', 5),\n",
       " ('断言', 5),\n",
       " ('其言', 5),\n",
       " ('自述', 5),\n",
       " ('之语', 5),\n",
       " ('并告诉', 5),\n",
       " ('写道', 5),\n",
       " ('讲', 5),\n",
       " ('提到', 4),\n",
       " ('要说', 4),\n",
       " ('宣称', 4),\n",
       " ('放过', 4),\n",
       " ('问他', 4),\n",
       " ('常说', 4),\n",
       " ('闻者', 4),\n",
       " ('默然', 4),\n",
       " ('深信', 4),\n",
       " ('责问', 4),\n",
       " ('坦言', 4),\n",
       " ('何如', 4),\n",
       " ('所讲', 4),\n",
       " ('之词', 4),\n",
       " ('看来', 4),\n",
       " ('并问', 4),\n",
       " ('中说', 4)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_end_marks():\n",
    "    end_marks = \"\"\"[\\u3000\\n\\r\\t@。？！?？|;!！【】]\"\"\"\n",
    "    return end_marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_in_quote_end_mark(string, end_marks):\n",
    "    quote_begin = ['‘', '“']\n",
    "    quote_end = ['’', '”']\n",
    "    replace_by = '.'\n",
    "    new_str = \"\"\n",
    "    start_replace = False\n",
    "    for c in string:\n",
    "        if c in quote_begin:\n",
    "            print('begin')\n",
    "            start_replace = True\n",
    "\n",
    "        if start_replace and c in end_marks:\n",
    "            print('end_marks')\n",
    "            new_str += replace_by\n",
    "        else:\n",
    "            new_str += c\n",
    "\n",
    "        if start_replace and c in quote_end:\n",
    "            print('quote_end')\n",
    "            start_replace = False\n",
    "\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_marks = r\"\"\"{}\"\"\".format(get_end_marks())\n",
    "sss = r'中国电科激光雷达总师屈恒阔激动地说：“作为货运飞船交会对接穿针引线的‘眼睛’！激光雷达在天宫二号【很大】与天舟一号交会对接过程中，确保了飞船空间自动交会对接、组合体飞行、绕飞等任务的成功实现。”'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n",
      "begin\n",
      "quote_end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'中国电科激光雷达总师屈恒阔激动地说：“作为货运飞船交会对接穿针引线的‘眼睛’！激光雷达在天宫二号【很大】与天舟一号交会对接过程中，确保了飞船空间自动交会对接、组合体飞行、绕飞等任务的成功实现。”'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_in_quote_end_mark(sss, end_marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\u3000\\n\\r\\t@。？！?？|;!！【】]'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_marks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
